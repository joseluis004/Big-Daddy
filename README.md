# üöÄ FinPlus Analytics Challenge: De Datos a Decisiones Accionables

<p align="center">
  <img src="https://img.shields.io/badge/Tecnolog√≠a-PySpark%2FDocker-blue" alt="PySpark Badge"/>
  <img src="https://img.shields.io/badge/Metodolog√≠a-Medallion%20Architecture-informational" alt="Medallion Badge"/>
  <img src="https://img.shields.io/badge/An√°lisis-Churn%20Score%20%2F%20RFM-success" alt="Analysis Badge"/>
  <img src="https://img.shields.io/badge/Status-Completado-brightgreen" alt="Status Badge"/>
</p>

---

## 1. üí° Resumen y Objetivo Estrat√©gico

Este repositorio contiene la soluci√≥n completa *end-to-end* desarrollada por **[NOMBRE DE SU EQUIPO / CONSULTORA]** para el "FinPlus Analytics Challenge".

Nuestro objetivo es transformar los datos crudos de clientes (demogr√°ficos, digitales y transaccionales) en una **inteligencia de negocio predictiva y accionable**.

### Objetivos Clave de la Soluci√≥n:

| # | Objetivo FinPlus | Resultado de la Soluci√≥n |
| :-: | :--- | :--- |
| **1** | Comprender Clientes | **Segmentaci√≥n Avanzada (Clustering)** para crear perfiles 360¬∞. |
| **2** | Detectar Riesgos | **Modelo Predictivo de Abandono (Churn Score)** asignado a cada cliente. |
| **3** | Oportunidades | **Features RFM** y m√©tricas de propensi√≥n para impulsar el *cross-selling*. |
| **4** | Toma de Decisiones | **Dashboard Ejecutivo** (Entregable 5) con KPIs claros y narrativos. |

## 2. ‚öôÔ∏è Arquitectura del Proyecto (Medallion Architecture)

El proyecto utiliza una arquitectura moderna de Big Data basada en capas de datos, garantizando **trazabilidad, calidad y rendimiento** con PySpark. 

[Image of Medallion data architecture diagram (Bronze, Silver, Gold layers)]


### 2.1. Estructura de Carpetas

| Carpeta | Contenido | Prop√≥sito |
| :--- | :--- | :--- |
| `data/` | `CLIENTS.csv`, `BEHAVIOURAL.csv` | Datos originales (Capa Bronce - Raw Data). |
| `docker/` | `Dockerfile`, `docker-compose.yml` | Configuraci√≥n del entorno de PySpark y JupyterLab. |
| `notebooks/` | `01_EDA.ipynb`, `02_Modelado.ipynb` | An√°lisis exploratorio y prototipado. |
| `src/` | `main_pipeline.py`, `feature_engineering.py` | **C√≥digo de Producci√≥n** (ETL, Limpieza y Funciones). |
| `docs/` | `Propuesta_Inicial.pdf`, `Diagrama_Arquitectura.pdf` | Documentaci√≥n formal del proyecto. |
| `dashboards/` | Capturas/Enlace del Dashboard | Visualizaci√≥n de los resultados. |

### 2.2. Roles del Equipo

| Miembro | Rol | Responsabilidad Principal |
| :--- | :--- | :--- |
| [Nombre 1] | **Project Lead (PL)** | Gesti√≥n de hitos, alineaci√≥n de negocio y presentaci√≥n de resultados. |
| [Nombre 2] | **Data Architect (DA)** | Dise√±o de arquitectura, entorno Docker y gobernanza de c√≥digo (GitHub). |
| [Nombre 3] | **Data Engineer (DE)** | Desarrollo del pipeline ETL con PySpark (Limpieza y Features). |
| [Nombre 4] | **Data Scientist (DS)** | An√°lisis avanzado (Fase 4.6), KPIs y dise√±o del Dashboard Ejecutivo. |

---

## 3. üõ†Ô∏è Gu√≠a de Instalaci√≥n y Ejecuci√≥n del Pipeline

Para replicar el entorno y ejecutar todo el an√°lisis (Entregables 4 y 6), siga estos sencillos pasos.

### Requisitos Previos

1.  **Git** (para clonar el repositorio).
2.  **Docker Desktop** (para el entorno reproducible).

### 3.1. Puesta en Marcha

1.  **Clonar el Repositorio:**
    ```bash
    git clone [https://github.com/](https://github.com/)[SuUsuario]/FinPlusAnalytics_TeamX.git
    cd FinPlusAnalytics_TeamX/
    ```

2.  **Navegar a la Carpeta Docker:**
    ```bash
    cd docker/
    ```

3.  **Construir el Entorno (Instala PySpark, Python, librer√≠as):**
    ```bash
    docker-compose build
    ```

4.  **Ejecutar el Contenedor (Inicia JupyterLab):**
    ```bash
    docker-compose up
    ```
    *Una vez iniciado, acceda a `http://localhost:8888` en su navegador.*

### 3.2. Ejecuci√≥n del Pipeline ETL y An√°lisis

Dentro de la terminal del contenedor o abriendo un Notebook de Jupyter, ejecute el script principal:

1.  **Detener el Servidor (Ctrl+C)** si est√° abierto.
2.  **Navegar a la Ra√≠z del Proyecto:**
    ```bash
    cd ..
    ```
3.  **Ejecutar el Pipeline Maestro:** (Asumiendo que `main_pipeline.py` orquesta la limpieza, features y modelos).
    ```bash
    docker exec -it finplus_analytics_container spark-submit src/main_pipeline.py
    ```

    * **Resultado:** Este comando generar√° las tablas de la **Capa Oro** final y los datasets para el dashboard, completando el **Entregable 4 y 6**.

---

## 4. üîó Entregables y Resultados

| Entregable | Contenido | Ubicaci√≥n |
| :--- | :--- | :--- |
| **Entregable 1 & 2** | Propuesta Inicial y Diagrama de Arquitectura. | `docs/` |
| **Entregable 3** | Evidencia de Commits y PRs (Revisar **Historial de GitHub**). | GitHub |
| **Entregable 4** | C√≥digo ETL (Limpieza y Features). | `src/` |
| **Entregable 5** | Dashboard Ejecutivo (Visualizaci√≥n). | [**LINK AL DASHBOARD** (Tableau/PowerBI)] |
| **Entregable 6 (Opcional)** | C√≥digo de Modelado (Clustering, Churn Score). | `src/` |

**¬°Gracias por su tiempo! Esperamos convertirnos en su socio anal√≠tico 2025.**