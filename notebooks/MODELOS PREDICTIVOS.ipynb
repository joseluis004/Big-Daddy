{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0dfaee",
   "metadata": {},
   "source": [
    "# MODELOS PREDICTIVOS CON MACHINE/DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bce831bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados. Shape: (322156, 49)\n",
      "TARGET_COL detectado automáticamente: 'NON_COMPLIANT_CONTRACT'\n",
      "Train/Test shapes: (257724, 48) (64432, 48)\n",
      "Numéricas: 36 / Categóricas: 12\n",
      "scale_pos_weight calculado: 11.280\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=100, model__subsample=0.6; total time=  11.6s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=100, model__subsample=0.6; total time=   7.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=100, model__subsample=0.6; total time=   7.4s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  12.7s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  13.5s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  15.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time=   9.0s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time=   9.0s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time=   9.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  11.9s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  11.2s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  10.2s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=400, model__subsample=1.0; total time=  13.2s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=400, model__subsample=1.0; total time=  13.5s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=400, model__subsample=1.0; total time=  13.5s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.03, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  12.7s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.03, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  12.9s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.03, model__max_depth=5, model__n_estimators=400, model__subsample=0.6; total time=  13.0s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100, model__subsample=1.0; total time=   7.9s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100, model__subsample=1.0; total time=   7.6s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100, model__subsample=1.0; total time=   7.7s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   7.4s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   7.4s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   7.3s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=400, model__subsample=1.0; total time=  13.9s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=400, model__subsample=1.0; total time=  14.3s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=400, model__subsample=1.0; total time=  15.0s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time=   6.9s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time=   6.9s\n",
      "[CV] END model__colsample_bytree=0.5, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time=   9.7s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   8.0s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   8.3s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=4, model__n_estimators=200, model__subsample=0.6; total time=   7.7s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  10.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  10.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.03, model__max_depth=6, model__n_estimators=200, model__subsample=0.8; total time=  10.7s\n",
      "Mejor params (RandomizedSearchCV): {'model__colsample_bytree': 0.7, 'model__gamma': 0, 'model__learning_rate': 0.03, 'model__max_depth': 6, 'model__n_estimators': 400, 'model__subsample': 1.0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 204\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Creamos nuevo XGB con esos params y early stopping\u001b[39;00m\n\u001b[1;32m    193\u001b[0m xgb_final \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m    194\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m     use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params_for_xgb\n\u001b[1;32m    202\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[43mxgb_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tr_sub_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_tr_sub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    210\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# 11) Predicción sobre test y métricas\u001b[39;00m\n\u001b[1;32m    213\u001b[0m y_prob_test \u001b[38;5;241m=\u001b[39m xgb_final\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_t)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# Versión robusta para evitar TerminatedWorkerError / OOM en GridSearch\n",
    "# Pega en tu notebook (ajusta FILE_PATH si quieres)\n",
    "# Requisitos: pip install xgboost scikit-learn pandas joblib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from scipy import stats\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FILE_PATH = \"/home/jovyan/work/data/curated/\"\n",
    "TARGET_COL = None   # si conoces el nombre, ponlo aquí\n",
    "APPROVAL_THRESHOLD = 0.8   # por defecto 80% (si quieres 0.8% usar 0.008)\n",
    "MODEL_OUTPUT = \"xgb_loan_model_robust.joblib\"\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Cargar datos\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"Archivo no encontrado en FILE_PATH: {FILE_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(FILE_PATH)\n",
    "print(\"Datos cargados. Shape:\", df.shape)\n",
    "\n",
    "# 2) Detectar target automaticamente (si TARGET_COL es None)\n",
    "if TARGET_COL is None:\n",
    "    candidates = [\"target\", \"y\", \"paid\", \"repaid\", \"repaid_flag\", \"loan_status\", \"default\", \"is_default\",\n",
    "                  \"paid_loan\", \"status\", \"GOOD_BAD\", \"good_bad\"]\n",
    "    lower_cols = {c.lower(): c for c in df.columns}\n",
    "    found = None\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower_cols:\n",
    "            found = lower_cols[c.lower()]\n",
    "            break\n",
    "    if found is None:\n",
    "        # intenta detectar columnas binarias 0/1\n",
    "        for col in df.columns:\n",
    "            vals = pd.Series(df[col].dropna().unique())\n",
    "            if set(vals.unique()).issubset({0,1}) and len(vals.unique())<=2:\n",
    "                found = col\n",
    "                break\n",
    "    if found is None:\n",
    "        raise ValueError(\n",
    "            \"No se ha detectado la columna objetivo. Asigna TARGET_COL manualmente al nombre de la columna 0/1.\"\n",
    "        )\n",
    "    TARGET_COL = found\n",
    "    print(f\"TARGET_COL detectado automáticamente: '{TARGET_COL}'\")\n",
    "\n",
    "# 3) Preparar X e y\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "# 4) Train/test split (estratificado si posible)\n",
    "stratify = y if len(np.unique(y))>1 else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify\n",
    ")\n",
    "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# 5) Detectar num y categ\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "print(f\"Numéricas: {len(num_cols)} / Categóricas: {len(cat_cols)}\")\n",
    "\n",
    "# 6) Preprocesamiento (ordinal para categóricas -> reduce memoria)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# OrdinalEncoder para mantener columnas compactas (XGBoost maneja bien encoding ordinal)\n",
    "# usamos encoded_missing_value=-1 para desconocidos\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")),\n",
    "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 7) Configurar XGBoost (n_jobs=1 para evitar nested parallelism)\n",
    "# Calculamos scale_pos_weight para clase positiva si hay desbalance:\n",
    "pos = np.sum(y_train == 1)\n",
    "neg = np.sum(y_train == 0)\n",
    "scale_pos_weight = 1.0\n",
    "if pos > 0 and neg > 0:\n",
    "    scale_pos_weight = neg / pos\n",
    "    print(f\"scale_pos_weight calculado: {scale_pos_weight:.3f}\")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1,   # important: avoid parallel workers inside each joblib worker\n",
    "    tree_method=\"hist\",\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preproc\", preprocessor), (\"model\", xgb)])\n",
    "\n",
    "# 8) RandomizedSearchCV ligero (n_jobs=1 para evitar workers extra)\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": [100, 200, 400],\n",
    "    \"model__max_depth\": stats.randint(3, 8),\n",
    "    \"model__learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"model__gamma\": [0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,           # rápido: prueba 12 combinaciones\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=2,\n",
    "    n_jobs=1,            # <-- importante: evita workers múltiples\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# 9) Fit con early stopping: pasamos fit_params prefijados con el nombre del paso ('model__')\n",
    "# Usamos eval_set con una porción del train para early stopping (aquí usamos X_test,y_test)\n",
    "fit_params = {\n",
    "    \"model__early_stopping_rounds\": 30,\n",
    "    \"model__eval_set\": [(preprocessor.fit_transform(X_test), y_test)]  # pre-transform test once for early stopping\n",
    "}\n",
    "# Nota: preprocessor.fit_transform(X_test) aplica transform con la preproc en su estado actual:\n",
    "# para evitar discrepancia entre folds, podríamos pasar eval_set con datos no transformados y dejar que pipeline lo haga.\n",
    "# Sin embargo RandomizedSearchCV pasará esos fit_params al pipeline y el prefijo 'model__eval_set' espera matrices\n",
    "# en la forma que XGBoost acepta (numpy/scipy). La transformación del eval_set la hacemos manualmente arriba para seguridad.\n",
    "\n",
    "# IMPORTANTE: si la línea anterior falla por transform en preproc no ajustado, hagamos fit parcial directo:\n",
    "# Para simplificar y robustez, en lugar de pasar eval_set pre-transformado, pasamos eval_set vacío y usamos early stopping en el modelo final.\n",
    "# Para entornos con problemas, puedes comentar fit_params y ejecutar rs.fit(X_train, y_train) sin early stopping en CV.\n",
    "\n",
    "# Vamos a intentar primero sin pasar eval_set a cada fit (más robusto en CV). En su lugar usaremos early stopping al final sobre validación.\n",
    "rs_fit_params = {}  # dejamos vacío para evitar errores durante CV\n",
    "\n",
    "try:\n",
    "    rs.fit(X_train, y_train, **rs_fit_params)\n",
    "except Exception as e:\n",
    "    print(\"Advertencia: RandomizedSearchCV con fit_params produjo un error. Reintentando sin fit_params.\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=8,\n",
    "        cv=3,\n",
    "        scoring=\"roc_auc\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=2,\n",
    "        n_jobs=1,\n",
    "        refit=True\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor params (RandomizedSearchCV):\", rs.best_params_)\n",
    "\n",
    "best_model = rs.best_estimator_\n",
    "\n",
    "# 10) Como good practice: reentrenamos el mejor XGB con early stopping sobre un validation set\n",
    "# Separamos del train un pequeño validation\n",
    "X_tr_sub, X_val, y_tr_sub, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train if len(np.unique(y_train))>1 else None)\n",
    "\n",
    "# Transformamos mediante preprocessor del pipeline (ya ajustado en rs.best_estimator_.named_steps['preproc'])\n",
    "preproc = best_model.named_steps[\"preproc\"]\n",
    "X_tr_sub_t = preproc.transform(X_tr_sub)\n",
    "X_val_t = preproc.transform(X_val)\n",
    "X_test_t = preproc.transform(X_test)\n",
    "\n",
    "# Extraemos parámetros encontrados\n",
    "best_params_for_xgb = {k.replace(\"model__\",\"\"): v for k,v in rs.best_params_.items() if k.startswith(\"model__\")}\n",
    "# Creamos nuevo XGB con esos params y early stopping\n",
    "xgb_final = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    **best_params_for_xgb\n",
    ")\n",
    "\n",
    "xgb_final.fit(\n",
    "    X_tr_sub_t,\n",
    "    y_tr_sub,\n",
    "    early_stopping_rounds=30,\n",
    "    eval_set=[(X_val_t, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 11) Predicción sobre test y métricas\n",
    "y_prob_test = xgb_final.predict_proba(X_test_t)[:, 1]\n",
    "y_pred_threshold = (y_prob_test >= APPROVAL_THRESHOLD).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(f\"\\nROC AUC en test: {auc:.4f}\")\n",
    "print(\"\\nClasificación con umbral (threshold = {:.4f}):\".format(APPROVAL_THRESHOLD))\n",
    "print(classification_report(y_test, y_pred_threshold))\n",
    "cm = confusion_matrix(y_test, y_pred_threshold)\n",
    "print(\"Matriz de confusión (verdadero x predicho):\\n\", cm)\n",
    "\n",
    "# 12) Añadir probabilidades y decisión al dataset de test y guardar\n",
    "X_test_out = X_test.reset_index(drop=True).copy()\n",
    "results = pd.DataFrame({\n",
    "    \"prob_pay\": y_prob_test,\n",
    "    \"approve\": y_pred_threshold,\n",
    "    \"actual\": y_test.reset_index(drop=True)\n",
    "})\n",
    "output = pd.concat([X_test_out, results], axis=1)\n",
    "output_file = \"test_with_probs_robust.csv\"\n",
    "output.to_csv(output_file, index=False)\n",
    "print(f\"\\nResultados de test guardados en {output_file}\")\n",
    "\n",
    "# 13) Guardar pipeline (preproc + modelo final integrado)\n",
    "# Construimos pipeline final: preproc + xgb_final (necesitamos un pipeline nuevo)\n",
    "final_pipeline = Pipeline(steps=[(\"preproc\", preproc), (\"model\", xgb_final)])\n",
    "joblib.dump(final_pipeline, MODEL_OUTPUT)\n",
    "print(f\"Pipeline final guardado en: {MODEL_OUTPUT}\")\n",
    "\n",
    "# 14) Mostrar top 10 por probabilidad\n",
    "print(\"\\nTop 10 clientes por probabilidad de pago (test set):\")\n",
    "display(output.sort_values(\"prob_pay\", ascending=False).head(10))\n",
    "\n",
    "# Ejemplo de uso posterior:\n",
    "print(f\"\"\"\n",
    "USO posterior:\n",
    "from joblib import load\n",
    "m = load('{MODEL_OUTPUT}')\n",
    "probs = m.predict_proba(nuevos_datos)[:,1]   # nuevos_datos debe tener las mismas columnas que X (sin la target)\n",
    "decisiones = (probs >= {APPROVAL_THRESHOLD}).astype(int)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e85833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. CARGA DE DATOS\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ruta al dataset curado\n",
    "df = pd.read_parquet(\"data/curated/Master_FinPlus_Final.parquet\")\n",
    "\n",
    "print(\"Dimensiones:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PREPARACIÓN DE VARIABLES\n",
    "# ============================================================\n",
    "\n",
    "# Variable objetivo:\n",
    "# NON_COMPLIANT_CONTRACT = 1 → No paga\n",
    "# Queremos probabilidad de PAGO → target = 1 si paga\n",
    "df[\"TARGET\"] = (df[\"NON_COMPLIANT_CONTRACT\"] == 0).astype(int)\n",
    "\n",
    "# Eliminamos columnas no útiles / fugas de información\n",
    "cols_drop = [\n",
    "    \"NON_COMPLIANT_CONTRACT\",\n",
    "    \"CLIENT_ID\",\n",
    "    \"CONTRACT_ID\",\n",
    "    \"DATE\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in cols_drop if c in df.columns])\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Detectar numéricas y categóricas\n",
    "# =======================\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Numéricas:\", len(numeric_cols))\n",
    "print(\"Categóricas:\", len(categorical_cols))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. PREPROCESAMIENTO\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = df.drop(\"TARGET\", axis=1)\n",
    "y = df[\"TARGET\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# One-hot-encoding para variables categóricas\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODELO XGBOOST\n",
    "# ============================================================\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Modelo entrenado correctamente.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. PROBABILIDAD DE PAGO & DECISIÓN DE CRÉDITO\n",
    "# ============================================================\n",
    "\n",
    "# Probabilidad de pagar (TARGET = 1)\n",
    "df[\"PROB_PAGO\"] = pipeline.predict_proba(X)[:, 1]\n",
    "\n",
    "# Decisión: se aprueba si PROB_PAGO ≥ 0.8\n",
    "df[\"APROBADO\"] = (df[\"PROB_PAGO\"] >= 0.8).astype(int)\n",
    "\n",
    "df[[\"PROB_PAGO\", \"APROBADO\"]].head()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "df.to_csv(\"output_probabilidad_credito.csv\", index=False)\n",
    "print(\"Archivo exportado: output_probabilidad_credito.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44735f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "\n",
    "FILE_PATH = \"/home/jovyan/work/data/curated/\"\n",
    "\n",
    "# Aquí cargas tu dataset final (ajusta el nombre si es distinto)\n",
    "df = pd.read_csv(FILE_PATH + \"final_df.csv\")  \n",
    "\n",
    "\n",
    "TARGET = \"NON_COMPLIANT_CONTRACT\"\n",
    "\n",
    "y = df[TARGET]\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.20, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        early_stopping_rounds=30,\n",
    "        tree_method=\"hist\",\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(\n",
    "        X_train, y_train,\n",
    "        model__eval_set=[(X_val, y_val)],\n",
    "        model__verbose=False\n",
    "    )\n",
    "\n",
    "    y_pred_prob = pipe.predict_proba(X_val)[:, 1]\n",
    "    return roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "xgb_final = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=30,\n",
    "    tree_method=\"hist\",\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "pipe_final = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", xgb_final)\n",
    "])\n",
    "\n",
    "pipe_final.fit(\n",
    "    X_train, y_train,\n",
    "    model__eval_set=[(X_val, y_val)],\n",
    "    model__verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "y_prob_test = pipe_final.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = (y_prob_test > 0.5).astype(int)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_prob_test)\n",
    "ks = np.max(tpr - fpr)\n",
    "print(\"KS:\", ks)\n",
    "\n",
    "\n",
    "model_inner = pipe_final.named_steps[\"model\"]\n",
    "pre = pipe_final.named_steps[\"prep\"]\n",
    "\n",
    "oh = pre.named_transformers_[\"cat\"].named_steps[\"oh\"]\n",
    "cat_names = oh.get_feature_names_out(cat_cols)\n",
    "final_features = np.concatenate([num_cols, cat_names])\n",
    "\n",
    "fimp = pd.DataFrame({\n",
    "    \"feature\": final_features,\n",
    "    \"importance\": model_inner.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "fimp.head(20)\n",
    "\n",
    "\n",
    "joblib.dump(pipe_final, FILE_PATH + \"xgb_model_final.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
