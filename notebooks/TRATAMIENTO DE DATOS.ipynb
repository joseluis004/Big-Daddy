{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7887eb35",
   "metadata": {},
   "source": [
    "# TRATAMIENTO DE DATOS CON PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42038e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURACIÓN E IMPORTACIONES\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Iniciamos Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinPlus_ETL_Limpieza\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Rutas de los datos\n",
    "DATA_PATH = \"/home/jovyan/work/data/\"  \n",
    "OUTPUT_PATH = \"/home/jovyan/work/data/curated/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc0c06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema Clients:\n",
      "root\n",
      " |-- CLIENT_ID: string (nullable = true)\n",
      " |-- NON_COMPLIANT_CONTRACT: integer (nullable = true)\n",
      " |-- NAME_PRODUCT_TYPE: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- TOTAL_INCOME: double (nullable = true)\n",
      " |-- AMOUNT_PRODUCT: double (nullable = true)\n",
      " |-- INSTALLMENT: double (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- HOME_SITUATION: string (nullable = true)\n",
      " |-- REGION_SCORE: double (nullable = true)\n",
      " |-- AGE_IN_YEARS: double (nullable = true)\n",
      " |-- JOB_SENIORITY: double (nullable = true)\n",
      " |-- HOME_SENIORITY: double (nullable = true)\n",
      " |-- LAST_UPDATE: double (nullable = true)\n",
      " |-- OWN_INSURANCE_CAR: string (nullable = true)\n",
      " |-- CAR_AGE: double (nullable = true)\n",
      " |-- FAMILY_SIZE: double (nullable = true)\n",
      " |-- REACTIVE_SCORING: double (nullable = true)\n",
      " |-- PROACTIVE_SCORING: double (nullable = true)\n",
      " |-- BEHAVIORAL_SCORING: double (nullable = true)\n",
      " |-- DAYS_LAST_INFO_CHANGE: double (nullable = true)\n",
      " |-- NUMBER_OF_PRODUCTS: double (nullable = true)\n",
      " |-- OCCUPATION: string (nullable = true)\n",
      " |-- DIGITAL_CLIENT: integer (nullable = true)\n",
      " |-- HOME_OWNER: string (nullable = true)\n",
      " |-- EMPLOYER_ORGANIZATION_TYPE: string (nullable = true)\n",
      " |-- CURRENCY: string (nullable = true)\n",
      " |-- NUM_PREVIOUS_LOAN_APP: double (nullable = true)\n",
      " |-- LOAN_ANNUITY_PAYMENT_MAX: double (nullable = true)\n",
      " |-- LOAN_ANNUITY_PAYMENT_MIN: double (nullable = true)\n",
      " |-- LOAN_ANNUITY_PAYMENT_SUM: double (nullable = true)\n",
      " |-- LOAN_APPLICATION_AMOUNT_MAX: double (nullable = true)\n",
      " |-- LOAN_APPLICATION_AMOUNT_MIN: double (nullable = true)\n",
      " |-- LOAN_APPLICATION_AMOUNT_SUM: double (nullable = true)\n",
      " |-- LOAN_CREDIT_GRANTED_MAX: double (nullable = true)\n",
      " |-- LOAN_CREDIT_GRANTED_MIN: double (nullable = true)\n",
      " |-- LOAN_CREDIT_GRANTED_SUM: double (nullable = true)\n",
      " |-- LOAN_VARIABLE_RATE_MAX: double (nullable = true)\n",
      " |-- LOAN_VARIABLE_RATE_MIN: double (nullable = true)\n",
      " |-- NUM_STATUS_ANNULLED: double (nullable = true)\n",
      " |-- NUM_STATUS_AUTHORIZED: double (nullable = true)\n",
      " |-- NUM_STATUS_DENIED: double (nullable = true)\n",
      " |-- NUM_STATUS_NOT_USED: double (nullable = true)\n",
      " |-- NUM_FLAG_INSURED: double (nullable = true)\n",
      "\n",
      "Esquema Behavioural:\n",
      "root\n",
      " |-- CONTRACT_ID: string (nullable = true)\n",
      " |-- CLIENT_ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- CREDICT_CARD_BALANCE: double (nullable = true)\n",
      " |-- CREDIT_CARD_LIMIT: double (nullable = true)\n",
      " |-- CREDIT_CARD_DRAWINGS_ATM: double (nullable = true)\n",
      " |-- CREDIT_CARD_DRAWINGS: double (nullable = true)\n",
      " |-- CREDIT_CARD_DRAWINGS_POS: double (nullable = true)\n",
      " |-- CREDIT_CARD_DRAWINGS_OTHER: double (nullable = true)\n",
      " |-- CREDIT_CARD_PAYMENT: double (nullable = true)\n",
      " |-- NUMBER_DRAWINGS_ATM: double (nullable = true)\n",
      " |-- NUMBER_DRAWINGS: long (nullable = true)\n",
      " |-- NUMBER_INSTALMENTS: double (nullable = true)\n",
      " |-- CURRENCY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. INGESTA DE DATOS \n",
    "\n",
    "# Cargar CLIENTS.csv\n",
    "# Usamos header=True para leer la cabecera e inferSchema=True para detectar números\n",
    "df_clients = spark.read.csv(DATA_PATH + \"CLIENTS.csv\", header=True, inferSchema=True, sep=',')\n",
    "\n",
    "# Cargar BEHAVIOURAL.parquet\n",
    "df_behav = spark.read.parquet(DATA_PATH + \"BEHAVIOURAL.parquet\")\n",
    "\n",
    "# Verificamos qué columnas tenemos \n",
    "print(\"Esquema Clients:\")\n",
    "df_clients.printSchema()\n",
    "\n",
    "print(\"Esquema Behavioural:\")\n",
    "df_behav.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cda49f",
   "metadata": {},
   "source": [
    "Primero analizamos las variables de tipo string. Hay de distintos tipos:\n",
    "\n",
    "- Categóricas puras: \"NAME_PRODUCT_TYPE\", \"GENDER\", \"EDUCATION\", \"MARITAL_STATUS\", \n",
    "    \"HOME_SITUATION\", \"OWN_INSURANCE_CAR\", \"OCCUPATION\", \n",
    "    \"HOME_OWNER\", \"EMPLOYER_ORGANIZATION_TYPE\", \"CURRENCY\" (en ambas tablas).\n",
    "\n",
    "- Categóricas numéricas: \"NON_COMPLIANT_CONTRACT\", \"DIGITAL_CLIENT\".\n",
    "\n",
    "- Fecha: \"DATE\" (sólo en behavioural).\n",
    "\n",
    "- Identificadores: \"CLIENT_ID\" (en ambas tablas), \"CONTRACT_ID\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9860",
   "metadata": {},
   "source": [
    "Analizamos las filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1152ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICACIÓN DE DUPLICADOS: CLIENTS\n",
      " \n",
      "• Total filas:      162977\n",
      "• Filas únicas:     162977\n",
      "• Duplicados:       0\n",
      "\n",
      "Limpio. No existen filas duplicadas exactas.\n",
      "VERIFICACIÓN DE DUPLICADOS: BEHAVIOURAL\n",
      " \n",
      "• Total filas:      1724854\n",
      "• Filas únicas:     1724854\n",
      "• Duplicados:       0\n",
      "\n",
      "Limpio. No existen filas duplicadas exactas.\n"
     ]
    }
   ],
   "source": [
    "def auditar_duplicados_completo(df, nombre_tabla):\n",
    "    print(f\"VERIFICACIÓN DE DUPLICADOS: {nombre_tabla}\\n \")\n",
    "    \n",
    "    # 1. Cálculos Básicos\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.distinct().count()\n",
    "    num_duplicados = total_rows - distinct_rows\n",
    "    \n",
    "    print(f\"• Total filas:      {total_rows}\")\n",
    "    print(f\"• Filas únicas:     {distinct_rows}\")\n",
    "    print(f\"• Duplicados:       {num_duplicados}\")\n",
    "    \n",
    "    # 2. Lógica Condicional\n",
    "    if num_duplicados > 0:\n",
    "        pct = (num_duplicados / total_rows) * 100\n",
    "        print(f\"\\n AVISO: Hay {num_duplicados} filas repetidas ({pct:.2f}%).\")\n",
    "        print(\"   Mostrando ejemplos de filas idénticas:\")\n",
    "        \n",
    "        # Esta parte solo se ejecuta si hay duplicados \n",
    "        # Agrupamos por TODAS las columnas para encontrar filas 100% idénticas\n",
    "        (df.groupBy(df.columns)\n",
    "           .count()\n",
    "           .where(F.col(\"count\") > 1)\n",
    "           .orderBy(F.col(\"count\").desc()) # Ponemos las más repetidas arriba\n",
    "           .show(5, truncate=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nLimpio. No existen filas duplicadas exactas.\")\n",
    "        \n",
    "\n",
    "# --- EJECUCIÓN ---\n",
    "auditar_duplicados_completo(df_clients, \"CLIENTS\")\n",
    "auditar_duplicados_completo(df_behav, \"BEHAVIOURAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b78be5",
   "metadata": {},
   "source": [
    "Ahora analizamos los posibles IDs duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8e0f7a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICACIÓN DE CLAVE ÚNICA (CLIENT_ID): CLIENTS\n",
      " \n",
      "CORRECTO: La columna CLIENT_ID es una clave primaria única.\n"
     ]
    }
   ],
   "source": [
    "def auditar_clave_primaria(df, col_id, nombre_tabla):\n",
    "    print(f\"VERIFICACIÓN DE CLAVE ÚNICA ({col_id}): {nombre_tabla}\\n \")\n",
    "    \n",
    "    # 1. Contamos filas totales\n",
    "    total = df.count()\n",
    "    \n",
    "    # 2. Contamos IDs únicos\n",
    "    unicos = df.select(col_id).distinct().count()\n",
    "    \n",
    "    # 3. Diferencia\n",
    "    dif = total - unicos\n",
    "    \n",
    "    if dif > 0:\n",
    "        print(f\"AVISO: Hay {dif} IDs repetidos que NO son filas idénticas.\")\n",
    "        print(\"   Esto significa que tienes clientes con datos conflictivos.\")\n",
    "        print(\"   Ejemplo de IDs repetidos:\")\n",
    "        \n",
    "        # Mostramos cuáles son los culpables\n",
    "        (df.groupBy(col_id)\n",
    "           .count()\n",
    "           .where(F.col(\"count\") > 1)\n",
    "           .show(5))\n",
    "    else:\n",
    "        print(f\"CORRECTO: La columna {col_id} es una clave primaria única.\")\n",
    "\n",
    "# Ejecutamos solo para CLIENTS\n",
    "auditar_clave_primaria(df_clients, \"CLIENT_ID\", \"CLIENTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b787f3",
   "metadata": {},
   "source": [
    "Como vemos que no hay duplicados, no es necesario hacer limpieza de estos. Nos enfocaremos en los NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa06436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECCIÓN DE: CLIENTS\n",
      "\n",
      "Dimensiones: 162977 filas x 45 columnas\n",
      "\n",
      "--- 1. Conteo de Nulos ---\n",
      "-RECORD 0-----------------------------\n",
      " CLIENT_ID                   | 0      \n",
      " NON_COMPLIANT_CONTRACT      | 0      \n",
      " NAME_PRODUCT_TYPE           | 0      \n",
      " GENDER                      | 0      \n",
      " TOTAL_INCOME                | 0      \n",
      " AMOUNT_PRODUCT              | 0      \n",
      " INSTALLMENT                 | 7      \n",
      " EDUCATION                   | 39640  \n",
      " MARITAL_STATUS              | 2      \n",
      " HOME_SITUATION              | 0      \n",
      " REGION_SCORE                | 0      \n",
      " AGE_IN_YEARS                | 0      \n",
      " JOB_SENIORITY               | 29174  \n",
      " HOME_SENIORITY              | 0      \n",
      " LAST_UPDATE                 | 0      \n",
      " OWN_INSURANCE_CAR           | 0      \n",
      " CAR_AGE                     | 107550 \n",
      " FAMILY_SIZE                 | 2      \n",
      " REACTIVE_SCORING            | 91901  \n",
      " PROACTIVE_SCORING           | 337    \n",
      " BEHAVIORAL_SCORING          | 32246  \n",
      " DAYS_LAST_INFO_CHANGE       | 1      \n",
      " NUMBER_OF_PRODUCTS          | 21903  \n",
      " OCCUPATION                  | 0      \n",
      " DIGITAL_CLIENT              | 0      \n",
      " HOME_OWNER                  | 0      \n",
      " EMPLOYER_ORGANIZATION_TYPE  | 29464  \n",
      " CURRENCY                    | 0      \n",
      " NUM_PREVIOUS_LOAN_APP       | 8770   \n",
      " LOAN_ANNUITY_PAYMENT_MAX    | 8770   \n",
      " LOAN_ANNUITY_PAYMENT_MIN    | 8770   \n",
      " LOAN_ANNUITY_PAYMENT_SUM    | 8770   \n",
      " LOAN_APPLICATION_AMOUNT_MAX | 8770   \n",
      " LOAN_APPLICATION_AMOUNT_MIN | 8770   \n",
      " LOAN_APPLICATION_AMOUNT_SUM | 8770   \n",
      " LOAN_CREDIT_GRANTED_MAX     | 8770   \n",
      " LOAN_CREDIT_GRANTED_MIN     | 8770   \n",
      " LOAN_CREDIT_GRANTED_SUM     | 8770   \n",
      " LOAN_VARIABLE_RATE_MAX      | 8770   \n",
      " LOAN_VARIABLE_RATE_MIN      | 8770   \n",
      " NUM_STATUS_ANNULLED         | 8770   \n",
      " NUM_STATUS_AUTHORIZED       | 8770   \n",
      " NUM_STATUS_DENIED           | 8770   \n",
      " NUM_STATUS_NOT_USED         | 8770   \n",
      " NUM_FLAG_INSURED            | 8770   \n",
      "\n",
      "INSPECCIÓN DE: BEHAVIOURAL\n",
      "\n",
      "Dimensiones: 1724854 filas x 14 columnas\n",
      "\n",
      "--- 1. Conteo de Nulos ---\n",
      "-RECORD 0-------------------------\n",
      " CONTRACT_ID                | 0   \n",
      " CLIENT_ID                  | 0   \n",
      " DATE                       | 0   \n",
      " CREDICT_CARD_BALANCE       | 0   \n",
      " CREDIT_CARD_LIMIT          | 0   \n",
      " CREDIT_CARD_DRAWINGS_ATM   | 0   \n",
      " CREDIT_CARD_DRAWINGS       | 0   \n",
      " CREDIT_CARD_DRAWINGS_POS   | 0   \n",
      " CREDIT_CARD_DRAWINGS_OTHER | 0   \n",
      " CREDIT_CARD_PAYMENT        | 0   \n",
      " NUMBER_DRAWINGS_ATM        | 0   \n",
      " NUMBER_DRAWINGS            | 0   \n",
      " NUMBER_INSTALMENTS         | 0   \n",
      " CURRENCY                   | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Definimos la función mejorada (le añadimos un título para que quede claro)\n",
    "def inspeccionar_datos(df, nombre_tabla):\n",
    "    \n",
    "    print(f\"INSPECCIÓN DE: {nombre_tabla}\\n\")\n",
    "    \n",
    "    print(f\"Dimensiones: {df.count()} filas x {len(df.columns)} columnas\")\n",
    "    \n",
    "    print(f\"\\n--- 1. Conteo de Nulos ---\")\n",
    "    # Calculamos nulos\n",
    "    exprs_nulos = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "    # Mostramos verticalmente para leer mejor\n",
    "    df.agg(*exprs_nulos).show(vertical=True, truncate=False)\n",
    "\n",
    "# 2. Ejecutamos la inspección \n",
    "inspeccionar_datos(df_clients, \"CLIENTS\")\n",
    "inspeccionar_datos(df_behav, \"BEHAVIOURAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c995d4",
   "metadata": {},
   "source": [
    "Se observa que solamente tendremos que tratar nulls de la tabla de clientes. Pero además, no nos interesa eliminar filas porque eliminaríamos clientes, y menos en variables con tantos nulls como CAR_AGE, donde estaríamos eliminando más del 66% de los clientes. La estrategia ganadora en Big Data es \"Imputar lo masivo, borrar lo anecdótico\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba9c20",
   "metadata": {},
   "source": [
    "Primero de todo, eliminaremos a los clientes de los que no se tengan casi datos, pues prácticamente es como si no existiesen. Comprobamos primero si, aunque no tengamos info personal tienen movimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe1706df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectados 1899 clientes 'Zombie' candidatos a borrar.\n",
      "\n",
      "AVISO: Hay 6863 clientes con pocos datos pero con movimientos.\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos de nuevo CLIENTS original (solo para esta prueba)\n",
    "df_raw = spark.read.csv(DATA_PATH + \"CLIENTS.csv\", header=True, inferSchema=True, sep=',')\n",
    "\n",
    "# 2. Identificamos a los que vamos a borrar (los que tienen muchos nulos)\n",
    "# thresh=25 mantenía a los buenos. Así que buscamos lo contrario.\n",
    "# Para replicar la lógica inversa exacta, calculamos cuántos nulos tienen.\n",
    "\n",
    "# Contamos cuántas columnas NO son nulas por fila\n",
    "from itertools import chain\n",
    "cols_check = df_raw.columns\n",
    "expr = sum([F.when(F.col(c).isNotNull(), 1).otherwise(0) for c in cols_check])\n",
    "\n",
    "# Filtramos los \"Malos\" (tienen menos de 25 columnas con datos)\n",
    "df_zombies = df_raw.withColumn(\"non_nulls\", expr).filter(F.col(\"non_nulls\") < 25)\n",
    "\n",
    "print(f\"Detectados {df_zombies.count()} clientes 'Zombie' candidatos a borrar.\")\n",
    "\n",
    "# 3. CRUCE DE LA VERDAD\n",
    "# Cruzamos estos Zombies con Behavioural. \n",
    "# Si sale 0, tu decisión fue perfecta. Si sale algo, cuidado.\n",
    "zombies_con_dinero = df_zombies.join(df_behav, on=\"CLIENT_ID\", how=\"inner\")\n",
    "\n",
    "count_risk = zombies_con_dinero.count()\n",
    "\n",
    "if count_risk == 0:\n",
    "    print(f\"\\n Ninguno de los clientes eliminados tenía actividad bancaria.\")\n",
    "else:\n",
    "    print(f\"\\nAVISO: Hay {count_risk} clientes con pocos datos pero con movimientos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d27ad",
   "metadata": {},
   "source": [
    "Con este resultado, esperaremos a eliminarlos después del JOIN de las tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcee95d",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________\n",
    "\n",
    "####### ELIMINAR \"CLIENTES ZOMBIE\" (Filas con demasiados nulos)\n",
    "\n",
    "#######print(f\"Filas antes de limpieza fina: {df_clients.count()}\")\n",
    "\n",
    "######## Tenemos unas 45 columnas. Si a un cliente le faltan más de 20 datos, no nos sirve.\n",
    "######## thresh=25 significa: \"Mantener solo si tiene al menos 25 columnas con datos válidos\"\n",
    "#######df_clients = df_clients.dropna(thresh=25) \n",
    "\n",
    "#######print(f\"Filas tras limpieza fina: {df_clients.count()}\")\n",
    "________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b57614f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. LIMPIEZA Y TRANSFORMACIÓN\n",
    "\n",
    "# A) LIMPIEZA DE STRINGS (CLIENTS)\n",
    "# Lista de tus columnas categóricas reales (copiadas de tu esquema)\n",
    "cols_categ_puras = [\n",
    "    \"NAME_PRODUCT_TYPE\", \"GENDER\", \"EDUCATION\", \"MARITAL_STATUS\", \n",
    "    \"HOME_SITUATION\", \"OWN_INSURANCE_CAR\", \"OCCUPATION\", \n",
    "    \"HOME_OWNER\", \"EMPLOYER_ORGANIZATION_TYPE\", \"CURRENCY\"\n",
    "]\n",
    "\n",
    "# Normalizamos: quitamos espacios (trim) y pasamos a mayúsculas o minúsculas\n",
    "for col_name in cols_categ_puras:\n",
    "    # Solo si la columna existe en el dataframe\n",
    "    if col_name in df_clients.columns:\n",
    "        df_clients = df_clients.withColumn(col_name, F.trim(F.upper(F.col(col_name))))\n",
    "\n",
    "# B) CONVERSIÓN DE FECHAS (BEHAVIOURAL)\n",
    "# Tu columna DATE es string, hay que pasarla a formato fecha\n",
    "# Spark suele ser listo, pero si falla, prueba con formato específico ej: \"dd/MM/yyyy\"\n",
    "df_behav = df_behav.withColumn(\"DATE\", F.to_date(F.col(\"DATE\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf81f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza completada.\n"
     ]
    }
   ],
   "source": [
    "# C) LIMPIEZA DE NULOS \n",
    "\n",
    "# 1. GRUPO \"NO APLICA\" (Rellenar con -1)\n",
    "# Variables donde Nulo significa \"No tiene\" o \"No disponible\"\n",
    "# CAR_AGE (66% nulos), JOB_SENIORITY, SCORING...\n",
    "cols_flag = [\"CAR_AGE\", \"JOB_SENIORITY\", \"REACTIVE_SCORING\", \"BEHAVIORAL_SCORING\", \"PROACTIVE_SCORING\", \"DAYS_LAST_INFO_CHANGE\"]\n",
    "df_clients = df_clients.fillna(-1, subset=cols_flag)\n",
    "\n",
    "# 2. GRUPO \"SIN HISTORIAL\" (Rellenar con 0)\n",
    "# El grupo de los 8770 nulos. Si no hay datos de préstamos, asumimos 0.\n",
    "# Buscamos todas las columnas de préstamos (LOAN_) y estados (NUM_STATUS_)\n",
    "cols_financieras = [c for c in df_clients.columns if c.startswith(\"LOAN_\") or c.startswith(\"NUM_\")]\n",
    "# Añadimos otras que tengan sentido ser 0\n",
    "cols_financieras.extend([\"NUM_PREVIOUS_LOAN_APP\", \"NUMBER_OF_PRODUCTS\", \"Num_flag_insured\"]) # Asegúrate de usar el nombre exacto (mayusc/minusc)\n",
    "\n",
    "# Filtramos solo las que existen en el DF para no dar error\n",
    "cols_financieras = [c for c in cols_financieras if c in df_clients.columns]\n",
    "df_clients = df_clients.fillna(0, subset=cols_financieras)\n",
    "\n",
    "# 3. GRUPO \"CATEGÓRICO DESCONOCIDO\" (Rellenar con 'Unknown')\n",
    "# EDUCATION tiene 39k nulos. No podemos inventárnosla.\n",
    "cols_categ_nulos = [\"EDUCATION\", \"EMPLOYER_ORGANIZATION_TYPE\", \"MARITAL_STATUS\"]\n",
    "df_clients = df_clients.fillna(\"UNKNOWN\", subset=cols_categ_nulos)\n",
    "\n",
    "# 4. GRUPO \"ANECDÓTICO\" (Estrategia: Salvar al Cliente)\n",
    "# Al ser poquísimos nulos, preferimos imputar para no perder la ficha del cliente.\n",
    "# Para Numéricas (Installment): Usamos la MEDIANA (más robusta que la media)\n",
    "# Calculamos la mediana aproximada (approxQuantile es muy eficiente en Spark)\n",
    "cols_anecdoticas_num = [\"INSTALLMENT\", \"FAMILY_SIZE\"]\n",
    "for col in cols_anecdoticas_num:\n",
    "    # Calculamos la mediana de esa columna específica\n",
    "    mediana = df_clients.stat.approxQuantile(col, [0.5], 0.01)[0]\n",
    "    df_clients = df_clients.fillna(mediana, subset=[col])\n",
    "\n",
    "print(\"Limpieza completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c72a266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECCIÓN DE: CLIENTS\n",
      "\n",
      "Dimensiones: 162977 filas x 45 columnas\n",
      "\n",
      "--- 1. Conteo de Nulos ---\n",
      "-RECORD 0--------------------------\n",
      " CLIENT_ID                   | 0   \n",
      " NON_COMPLIANT_CONTRACT      | 0   \n",
      " NAME_PRODUCT_TYPE           | 0   \n",
      " GENDER                      | 0   \n",
      " TOTAL_INCOME                | 0   \n",
      " AMOUNT_PRODUCT              | 0   \n",
      " INSTALLMENT                 | 0   \n",
      " EDUCATION                   | 0   \n",
      " MARITAL_STATUS              | 0   \n",
      " HOME_SITUATION              | 0   \n",
      " REGION_SCORE                | 0   \n",
      " AGE_IN_YEARS                | 0   \n",
      " JOB_SENIORITY               | 0   \n",
      " HOME_SENIORITY              | 0   \n",
      " LAST_UPDATE                 | 0   \n",
      " OWN_INSURANCE_CAR           | 0   \n",
      " CAR_AGE                     | 0   \n",
      " FAMILY_SIZE                 | 0   \n",
      " REACTIVE_SCORING            | 0   \n",
      " PROACTIVE_SCORING           | 0   \n",
      " BEHAVIORAL_SCORING          | 0   \n",
      " DAYS_LAST_INFO_CHANGE       | 0   \n",
      " NUMBER_OF_PRODUCTS          | 0   \n",
      " OCCUPATION                  | 0   \n",
      " DIGITAL_CLIENT              | 0   \n",
      " HOME_OWNER                  | 0   \n",
      " EMPLOYER_ORGANIZATION_TYPE  | 0   \n",
      " CURRENCY                    | 0   \n",
      " NUM_PREVIOUS_LOAN_APP       | 0   \n",
      " LOAN_ANNUITY_PAYMENT_MAX    | 0   \n",
      " LOAN_ANNUITY_PAYMENT_MIN    | 0   \n",
      " LOAN_ANNUITY_PAYMENT_SUM    | 0   \n",
      " LOAN_APPLICATION_AMOUNT_MAX | 0   \n",
      " LOAN_APPLICATION_AMOUNT_MIN | 0   \n",
      " LOAN_APPLICATION_AMOUNT_SUM | 0   \n",
      " LOAN_CREDIT_GRANTED_MAX     | 0   \n",
      " LOAN_CREDIT_GRANTED_MIN     | 0   \n",
      " LOAN_CREDIT_GRANTED_SUM     | 0   \n",
      " LOAN_VARIABLE_RATE_MAX      | 0   \n",
      " LOAN_VARIABLE_RATE_MIN      | 0   \n",
      " NUM_STATUS_ANNULLED         | 0   \n",
      " NUM_STATUS_AUTHORIZED       | 0   \n",
      " NUM_STATUS_DENIED           | 0   \n",
      " NUM_STATUS_NOT_USED         | 0   \n",
      " NUM_FLAG_INSURED            | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspeccionar_datos(df_clients, \"CLIENTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74c42e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FASE 1: JOIN Y DIAGNÓSTICO (NOMBRES ORIGINALES)\n",
      "\n",
      "Uniendo tablas por CLIENT_ID...\n",
      "CORRECTO: 1 Cliente = 1 Fila.\n",
      "Aplicando estándar bancario a fechas (Imputación 1900-01-01)...\n",
      " ESTADO ACTUAL (PRE-REDUCCIÓN)\n",
      "\n",
      "Dimensiones: 162977 filas x 59 columnas\n",
      "\n",
      "Esquema Actual \n",
      "root\n",
      " |-- CLIENT_ID: string (nullable = true)\n",
      " |-- NON_COMPLIANT_CONTRACT: integer (nullable = true)\n",
      " |-- NAME_PRODUCT_TYPE: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- TOTAL_INCOME: double (nullable = true)\n",
      " |-- AMOUNT_PRODUCT: double (nullable = true)\n",
      " |-- INSTALLMENT: double (nullable = false)\n",
      " |-- EDUCATION: string (nullable = false)\n",
      " |-- MARITAL_STATUS: string (nullable = false)\n",
      " |-- HOME_SITUATION: string (nullable = true)\n",
      " |-- REGION_SCORE: double (nullable = true)\n",
      " |-- AGE_IN_YEARS: double (nullable = true)\n",
      " |-- JOB_SENIORITY: double (nullable = false)\n",
      " |-- HOME_SENIORITY: double (nullable = true)\n",
      " |-- LAST_UPDATE: double (nullable = true)\n",
      " |-- OWN_INSURANCE_CAR: string (nullable = true)\n",
      " |-- CAR_AGE: double (nullable = false)\n",
      " |-- FAMILY_SIZE: double (nullable = false)\n",
      " |-- REACTIVE_SCORING: double (nullable = false)\n",
      " |-- PROACTIVE_SCORING: double (nullable = false)\n",
      " |-- BEHAVIORAL_SCORING: double (nullable = false)\n",
      " |-- DAYS_LAST_INFO_CHANGE: double (nullable = false)\n",
      " |-- NUMBER_OF_PRODUCTS: double (nullable = false)\n",
      " |-- OCCUPATION: string (nullable = true)\n",
      " |-- DIGITAL_CLIENT: integer (nullable = true)\n",
      " |-- HOME_OWNER: string (nullable = true)\n",
      " |-- EMPLOYER_ORGANIZATION_TYPE: string (nullable = false)\n",
      " |-- CURRENCY: string (nullable = true)\n",
      " |-- NUM_PREVIOUS_LOAN_APP: double (nullable = false)\n",
      " |-- LOAN_ANNUITY_PAYMENT_MAX: double (nullable = false)\n",
      " |-- LOAN_ANNUITY_PAYMENT_MIN: double (nullable = false)\n",
      " |-- LOAN_ANNUITY_PAYMENT_SUM: double (nullable = false)\n",
      " |-- LOAN_APPLICATION_AMOUNT_MAX: double (nullable = false)\n",
      " |-- LOAN_APPLICATION_AMOUNT_MIN: double (nullable = false)\n",
      " |-- LOAN_APPLICATION_AMOUNT_SUM: double (nullable = false)\n",
      " |-- LOAN_CREDIT_GRANTED_MAX: double (nullable = false)\n",
      " |-- LOAN_CREDIT_GRANTED_MIN: double (nullable = false)\n",
      " |-- LOAN_CREDIT_GRANTED_SUM: double (nullable = false)\n",
      " |-- LOAN_VARIABLE_RATE_MAX: double (nullable = false)\n",
      " |-- LOAN_VARIABLE_RATE_MIN: double (nullable = false)\n",
      " |-- NUM_STATUS_ANNULLED: double (nullable = false)\n",
      " |-- NUM_STATUS_AUTHORIZED: double (nullable = false)\n",
      " |-- NUM_STATUS_DENIED: double (nullable = false)\n",
      " |-- NUM_STATUS_NOT_USED: double (nullable = false)\n",
      " |-- NUM_FLAG_INSURED: double (nullable = false)\n",
      " |-- CONTRACT_ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- CREDICT_CARD_BALANCE: double (nullable = false)\n",
      " |-- CREDIT_CARD_LIMIT: double (nullable = false)\n",
      " |-- CREDIT_CARD_DRAWINGS_ATM: double (nullable = false)\n",
      " |-- CREDIT_CARD_DRAWINGS: double (nullable = false)\n",
      " |-- CREDIT_CARD_DRAWINGS_POS: double (nullable = false)\n",
      " |-- CREDIT_CARD_DRAWINGS_OTHER: double (nullable = false)\n",
      " |-- CREDIT_CARD_PAYMENT: double (nullable = false)\n",
      " |-- NUMBER_DRAWINGS_ATM: double (nullable = false)\n",
      " |-- NUMBER_DRAWINGS: long (nullable = true)\n",
      " |-- NUMBER_INSTALMENTS: double (nullable = false)\n",
      " |-- CURRENCY: string (nullable = true)\n",
      " |-- KPI_DAYS_LAST_MOV: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. INTEGRACIÓN (JOIN) - MODO NOMBRES ORIGINALES\n",
    "\n",
    "\n",
    "print(\" FASE 1: JOIN Y DIAGNÓSTICO (NOMBRES ORIGINALES)\\n\")\n",
    "\n",
    "col_id = \"CLIENT_ID\" \n",
    "\n",
    "# --- A. SNAPSHOT (Último dato por cliente) ---\n",
    "df_behav = df_behav.withColumn(\"DATE\", F.to_date(F.col(\"DATE\")))\n",
    "\n",
    "# Ventana para quedarnos con el último registro\n",
    "w = Window.partitionBy(col_id).orderBy(F.desc(\"DATE\"))\n",
    "df_behav_dedup = df_behav.withColumn(\"rank\", F.row_number().over(w)) \\\n",
    "                         .filter(F.col(\"rank\") == 1) \\\n",
    "                         .drop(\"rank\")\n",
    "\n",
    "# --- B. JOIN ---\n",
    "print(f\"Uniendo tablas por {col_id}...\")\n",
    "df_master = df_clients.join(df_behav_dedup, on=col_id, how=\"left\")\n",
    "\n",
    "# --- C. INTEGRIDAD ---\n",
    "filas_clientes = df_clients.count()\n",
    "filas_master = df_master.count()\n",
    "\n",
    "if filas_master > filas_clientes:\n",
    "    diff = filas_master - filas_clientes\n",
    "    print(f\"AVISO: Aún hay {diff} duplicados.\")\n",
    "else:\n",
    "    print(f\"CORRECTO: 1 Cliente = 1 Fila.\")\n",
    "\n",
    "# --- D. LIMPIEZA POST-JOIN (Nulos a 0) ---\n",
    "# Tras el join, las columnas financieras de BEHAVIOURAL pueden tener nulos. \n",
    "# En el contexto bancario y transaccional, la ausencia de registro significa cantidad cero.\n",
    "cols_financieras = [\n",
    "    \"CREDICT_CARD_BALANCE\", \"CREDIT_CARD_LIMIT\", \"NUMBER_INSTALMENTS\", \"NUMBER_DRAWINGS\", \n",
    "    \"NUMBER_DRAWINGS_ATM\",\"CREDIT_CARD_DRAWINGS_ATM\", \"CREDIT_CARD_DRAWINGS_POS\", \n",
    "    \"CREDIT_CARD_DRAWINGS_OTHER\", \"CREDIT_CARD_DRAWINGS\", \"CREDIT_CARD_PAYMENT\"\n",
    "]\n",
    "\n",
    "cols_existentes = [c for c in cols_financieras if c in df_master.columns]\n",
    "df_master = df_master.fillna(0, subset=cols_existentes)\n",
    "\n",
    "# --- E. GESTIÓN DE FECHAS (ESTRATEGIA 'SENTINEL VALUE') ---\n",
    "# En banca no se borran fechas, se imputa una fecha 'imposible' (1900-01-01).\n",
    "if \"DATE\" in df_master.columns:\n",
    "    print(\"Aplicando estándar bancario a fechas (Imputación 1900-01-01)...\")\n",
    "    \n",
    "    # A) Primero calculamos la RECENCIA (Días desde último mov)\n",
    "    #    Para esto usamos la fecha real antes de \"taparla\"\n",
    "    max_date = df_master.agg(F.max(\"DATE\")).collect()[0][0]\n",
    "    \n",
    "    df_master = df_master.withColumn(\n",
    "        \"KPI_DAYS_LAST_MOV\", \n",
    "        F.datediff(F.lit(max_date), F.col(\"DATE\"))\n",
    "    )\n",
    "    # Si es nulo (sin datos), ponemos 9999 días (inactivo histórico)\n",
    "    df_master = df_master.fillna(9999, subset=[\"KPI_DAYS_LAST_MOV\"])\n",
    "    \n",
    "    # B) Ahora aplicamos el SENTINEL VALUE a la columna original\n",
    "    #    Usamos 'coalesce': Si DATE es null, pon 1900-01-01.\n",
    "    df_master = df_master.withColumn(\n",
    "        \"DATE\",\n",
    "        F.coalesce(F.col(\"DATE\"), F.lit(\"1900-01-01\").cast(\"date\"))\n",
    "    )\n",
    "\n",
    "# F. INFORMACIÓN DE LA TABLA \n",
    "print(\" ESTADO ACTUAL (PRE-REDUCCIÓN)\\n\")\n",
    "print(f\"Dimensiones: {df_master.count()} filas x {len(df_master.columns)} columnas\")\n",
    "print(\"\\nEsquema Actual \")\n",
    "df_master.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "214d0491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASE 2: KPIs AVANZADOS Y LIMPIEZA FINAL\n",
      " \n",
      "Generando KPIs estratégicos...\n",
      "Eliminando columnas redundantes e inútiles...\n",
      "ESTADO FINAL OPTIMIZADO\n",
      "\n",
      "Dimensiones: 162977 filas x 50 columnas\n",
      "(Se han eliminado 14 columnas redundantes y creado 6 KPIs estratégicos)\n",
      "\n",
      "Guardado archivo en: /home/jovyan/work/data/curated/Master_FinPlus.parquet.\n",
      "\n",
      "Guardado.\n",
      " Ruta: /home/jovyan/work/data/curated/Master_FinPlus_Final.parquet\n",
      "root\n",
      " |-- CLIENT_ID: string (nullable = true)\n",
      " |-- NON_COMPLIANT_CONTRACT: integer (nullable = true)\n",
      " |-- NAME_PRODUCT_TYPE: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- TOTAL_INCOME: double (nullable = true)\n",
      " |-- AMOUNT_PRODUCT: double (nullable = true)\n",
      " |-- INSTALLMENT: double (nullable = false)\n",
      " |-- EDUCATION: string (nullable = false)\n",
      " |-- MARITAL_STATUS: string (nullable = false)\n",
      " |-- HOME_SITUATION: string (nullable = true)\n",
      " |-- REGION_SCORE: double (nullable = true)\n",
      " |-- AGE_IN_YEARS: double (nullable = true)\n",
      " |-- JOB_SENIORITY: double (nullable = false)\n",
      " |-- HOME_SENIORITY: double (nullable = true)\n",
      " |-- LAST_UPDATE: double (nullable = true)\n",
      " |-- OWN_INSURANCE_CAR: string (nullable = true)\n",
      " |-- CAR_AGE: double (nullable = false)\n",
      " |-- FAMILY_SIZE: double (nullable = false)\n",
      " |-- REACTIVE_SCORING: double (nullable = false)\n",
      " |-- PROACTIVE_SCORING: double (nullable = false)\n",
      " |-- BEHAVIORAL_SCORING: double (nullable = false)\n",
      " |-- DAYS_LAST_INFO_CHANGE: double (nullable = false)\n",
      " |-- NUMBER_OF_PRODUCTS: double (nullable = false)\n",
      " |-- OCCUPATION: string (nullable = true)\n",
      " |-- DIGITAL_CLIENT: integer (nullable = true)\n",
      " |-- HOME_OWNER: string (nullable = true)\n",
      " |-- EMPLOYER_ORGANIZATION_TYPE: string (nullable = false)\n",
      " |-- NUM_PREVIOUS_LOAN_APP: double (nullable = false)\n",
      " |-- LOAN_ANNUITY_PAYMENT_SUM: double (nullable = false)\n",
      " |-- LOAN_APPLICATION_AMOUNT_SUM: double (nullable = false)\n",
      " |-- LOAN_CREDIT_GRANTED_SUM: double (nullable = false)\n",
      " |-- NUM_STATUS_ANNULLED: double (nullable = false)\n",
      " |-- NUM_STATUS_AUTHORIZED: double (nullable = false)\n",
      " |-- NUM_STATUS_DENIED: double (nullable = false)\n",
      " |-- NUM_STATUS_NOT_USED: double (nullable = false)\n",
      " |-- NUM_FLAG_INSURED: double (nullable = false)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- CREDICT_CARD_BALANCE: double (nullable = false)\n",
      " |-- CREDIT_CARD_LIMIT: double (nullable = false)\n",
      " |-- CREDIT_CARD_PAYMENT: double (nullable = false)\n",
      " |-- NUMBER_DRAWINGS_ATM: double (nullable = false)\n",
      " |-- NUMBER_DRAWINGS: long (nullable = true)\n",
      " |-- NUMBER_INSTALMENTS: double (nullable = false)\n",
      " |-- KPI_DAYS_LAST_MOV: integer (nullable = true)\n",
      " |-- KPI_TOTAL_SPEND: double (nullable = false)\n",
      " |-- KPI_DEBT_RATIO: double (nullable = true)\n",
      " |-- KPI_AGE_GROUP: string (nullable = false)\n",
      " |-- KPI_LOAN_VOLATILITY: double (nullable = false)\n",
      " |-- KPI_APPROVAL_RATIO: double (nullable = true)\n",
      " |-- KPI_DENIAL_RATE: double (nullable = true)\n",
      "\n",
      "INSPECCIÓN DE: MASTER BOARD\n",
      "\n",
      "Dimensiones: 162977 filas x 50 columnas\n",
      "\n",
      "--- 1. Conteo de Nulos ---\n",
      "-RECORD 0--------------------------\n",
      " CLIENT_ID                   | 0   \n",
      " NON_COMPLIANT_CONTRACT      | 0   \n",
      " NAME_PRODUCT_TYPE           | 0   \n",
      " GENDER                      | 0   \n",
      " TOTAL_INCOME                | 0   \n",
      " AMOUNT_PRODUCT              | 0   \n",
      " INSTALLMENT                 | 0   \n",
      " EDUCATION                   | 0   \n",
      " MARITAL_STATUS              | 0   \n",
      " HOME_SITUATION              | 0   \n",
      " REGION_SCORE                | 0   \n",
      " AGE_IN_YEARS                | 0   \n",
      " JOB_SENIORITY               | 0   \n",
      " HOME_SENIORITY              | 0   \n",
      " LAST_UPDATE                 | 0   \n",
      " OWN_INSURANCE_CAR           | 0   \n",
      " CAR_AGE                     | 0   \n",
      " FAMILY_SIZE                 | 0   \n",
      " REACTIVE_SCORING            | 0   \n",
      " PROACTIVE_SCORING           | 0   \n",
      " BEHAVIORAL_SCORING          | 0   \n",
      " DAYS_LAST_INFO_CHANGE       | 0   \n",
      " NUMBER_OF_PRODUCTS          | 0   \n",
      " OCCUPATION                  | 0   \n",
      " DIGITAL_CLIENT              | 0   \n",
      " HOME_OWNER                  | 0   \n",
      " EMPLOYER_ORGANIZATION_TYPE  | 0   \n",
      " NUM_PREVIOUS_LOAN_APP       | 0   \n",
      " LOAN_ANNUITY_PAYMENT_SUM    | 0   \n",
      " LOAN_APPLICATION_AMOUNT_SUM | 0   \n",
      " LOAN_CREDIT_GRANTED_SUM     | 0   \n",
      " NUM_STATUS_ANNULLED         | 0   \n",
      " NUM_STATUS_AUTHORIZED       | 0   \n",
      " NUM_STATUS_DENIED           | 0   \n",
      " NUM_STATUS_NOT_USED         | 0   \n",
      " NUM_FLAG_INSURED            | 0   \n",
      " DATE                        | 0   \n",
      " CREDICT_CARD_BALANCE        | 0   \n",
      " CREDIT_CARD_LIMIT           | 0   \n",
      " CREDIT_CARD_PAYMENT         | 0   \n",
      " NUMBER_DRAWINGS_ATM         | 0   \n",
      " NUMBER_DRAWINGS             | 0   \n",
      " NUMBER_INSTALMENTS          | 0   \n",
      " KPI_DAYS_LAST_MOV           | 0   \n",
      " KPI_TOTAL_SPEND             | 0   \n",
      " KPI_DEBT_RATIO              | 0   \n",
      " KPI_AGE_GROUP               | 0   \n",
      " KPI_LOAN_VOLATILITY         | 0   \n",
      " KPI_APPROVAL_RATIO          | 0   \n",
      " KPI_DENIAL_RATE             | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. FEATURE ENGINEERING Y REDUCCIÓN FINAL\n",
    "\n",
    "print(\"FASE 2: KPIs AVANZADOS Y LIMPIEZA FINAL\\n \")\n",
    "\n",
    "# --- A. CREACIÓN DE KPIs (Información de Negocio) ---\n",
    "print(\"Generando KPIs estratégicos...\")\n",
    "\n",
    "# 1. KPI Gasto Total Tarjeta (Suma de los parciales)\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_TOTAL_SPEND\",\n",
    "    F.col(\"CREDIT_CARD_DRAWINGS_ATM\") + F.col(\"CREDIT_CARD_DRAWINGS_POS\") + F.col(\"CREDIT_CARD_DRAWINGS_OTHER\")\n",
    ")\n",
    "\n",
    "# 2. KPI Ratio Endeudamiento (Deuda / Ingresos)\n",
    "# Sumamos 1 al ingreso para evitar divisiones por cero\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_DEBT_RATIO\",\n",
    "    F.round(F.col(\"CREDICT_CARD_BALANCE\") / (F.col(\"TOTAL_INCOME\") + 1), 4)\n",
    ")\n",
    "\n",
    "# 3. KPI Grupo de Edad (Simplificación demográfica)\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_AGE_GROUP\",\n",
    "    F.when(F.col(\"AGE_IN_YEARS\") < 30, \"Joven\")\n",
    "     .when(F.col(\"AGE_IN_YEARS\") < 50, \"Adulto\")\n",
    "     .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "# 4. KPI Volatilidad de Préstamos (Max - Min)\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_LOAN_VOLATILITY\",\n",
    "    F.col(\"LOAN_CREDIT_GRANTED_MAX\") - F.col(\"LOAN_CREDIT_GRANTED_MIN\")\n",
    ")\n",
    "\n",
    "# 5. KPI Ratio de Aprobación (Lo que le dieron / Lo que pidió)\n",
    "# Indica la confianza del banco en el cliente\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_APPROVAL_RATIO\",\n",
    "    F.round(F.col(\"LOAN_CREDIT_GRANTED_SUM\") / (F.col(\"LOAN_APPLICATION_AMOUNT_SUM\") + 1), 2)\n",
    ")\n",
    "\n",
    "# 6. KPI Tasa de Rechazo (Solicitudes denegadas / Total)\n",
    "# Resume los estados conflictivos\n",
    "df_master = df_master.withColumn(\n",
    "    \"KPI_DENIAL_RATE\",\n",
    "    F.round(F.col(\"NUM_STATUS_DENIED\") / \n",
    "            (F.col(\"NUM_STATUS_AUTHORIZED\") + F.col(\"NUM_STATUS_DENIED\") + F.col(\"NUM_STATUS_ANNULLED\") + 1), 2)\n",
    ")\n",
    "\n",
    "# --- B. REDUCCIÓN DE COLUMNAS (Limpieza de \"Grasa\") ---\n",
    "print(\"Eliminando columnas redundantes e inútiles...\")\n",
    "\n",
    "cols_a_borrar = [\n",
    "    # 1. Redundantes de Tarjeta (Sustituidas por KPI_TOTAL_SPEND)\n",
    "    \"CREDIT_CARD_DRAWINGS_ATM\", \"CREDIT_CARD_DRAWINGS_POS\", \"CREDIT_CARD_DRAWINGS_OTHER\", \n",
    "    \"CREDIT_CARD_DRAWINGS\", # Dato duplicado original\n",
    "    \n",
    "    # 2. Redundantes de Préstamos (Sustituidas por KPI_LOAN_VOLATILITY y APPROVAL)\n",
    "    \"LOAN_ANNUITY_PAYMENT_MAX\", \"LOAN_ANNUITY_PAYMENT_MIN\",\n",
    "    \"LOAN_APPLICATION_AMOUNT_MAX\", \"LOAN_APPLICATION_AMOUNT_MIN\",\n",
    "    \"LOAN_CREDIT_GRANTED_MAX\", \"LOAN_CREDIT_GRANTED_MIN\",\n",
    "    \"LOAN_VARIABLE_RATE_MAX\", \"LOAN_VARIABLE_RATE_MIN\",\n",
    "\n",
    "    # 3. Basura Técnica y Duplicados\n",
    "    \"CONTRACT_ID\",  # ID interno técnico (inútil para negocio)\n",
    "    \"CURRENCY\"      # Columna duplicada por el Join (borramos ambas)\n",
    "    ]\n",
    "\n",
    "# Borrado seguro (solo las que existan)\n",
    "cols_finales_borrar = [c for c in cols_a_borrar if c in df_master.columns]\n",
    "df_master = df_master.drop(*cols_finales_borrar)\n",
    "\n",
    "# --- C. DIMENSIONES FINALES ---\n",
    "\n",
    "print(\"ESTADO FINAL OPTIMIZADO\\n\")\n",
    "print(f\"Dimensiones: {df_master.count()} filas x {len(df_master.columns)} columnas\")\n",
    "print(f\"(Se han eliminado {len(cols_finales_borrar)} columnas redundantes y creado 6 KPIs estratégicos)\")\n",
    "\n",
    "# --- D. GUARDADO FINAL ---\n",
    "ruta_final = OUTPUT_PATH + \"Master_FinPlus.parquet\"\n",
    "df_master.write.mode(\"overwrite\").parquet(ruta_final)\n",
    "print(f\"\\nGuardado archivo en: {ruta_final}.\")\n",
    "\n",
    "# --- E. GUARDADO EN PARQUET ÚNICO ---\n",
    "\n",
    "df_pandas = df_master.toPandas()\n",
    "\n",
    "nombre_archivo = \"Master_FinPlus_Final.parquet\"\n",
    "ruta_completa = OUTPUT_PATH + nombre_archivo\n",
    "\n",
    "# Necesitas tener instalada la librería pyarrow o fastparquet (suele venir en Docker)\n",
    "df_pandas.to_parquet(ruta_completa, index=False)\n",
    "\n",
    "print(f\"\\nGuardado.\")\n",
    "print(f\" Ruta: {ruta_completa}\")\n",
    "\n",
    "df_master.printSchema()\n",
    "inspeccionar_datos(df_master, \"MASTER BOARD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0439fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VERIFICACIÓN DE OUTLIERS (Método IQR)\n",
      "\n",
      " Columna: TOTAL_INCOME\n",
      "   Rango Normal: [-270.00  a  4050.00]\n",
      "   Outliers detectados: 7409 filas\n",
      "   Ejemplos (Top Extremos):\n",
      "+------------+\n",
      "|TOTAL_INCOME|\n",
      "+------------+\n",
      "|   1404000.0|\n",
      "|   216001.08|\n",
      "|    108000.0|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      " Columna: AGE_IN_YEARS -> Sin outliers estadísticos.\n",
      "\n",
      " Columna: AMOUNT_PRODUCT\n",
      "   Rango Normal: [-6455.70  a  19399.50]\n",
      "   Outliers detectados: 3465 filas\n",
      "   Ejemplos (Top Extremos):\n",
      "+--------------+\n",
      "|AMOUNT_PRODUCT|\n",
      "+--------------+\n",
      "|     48486.195|\n",
      "|     48486.195|\n",
      "|     48486.195|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      " Columna: CREDICT_CARD_BALANCE\n",
      "   Rango Normal: [0.00  a  0.00]\n",
      "   Outliers detectados: 16336 filas\n",
      "   Ejemplos (Top Extremos):\n",
      "+--------------------+\n",
      "|CREDICT_CARD_BALANCE|\n",
      "+--------------------+\n",
      "|            14526.13|\n",
      "|            12122.07|\n",
      "|            11500.21|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def auditar_outliers(df, cols_numericas):\n",
    "    \n",
    "    print(f\" VERIFICACIÓN DE OUTLIERS (Método IQR)\")\n",
    "    \n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        # 1. Calculamos Cuartiles (25% y 75%)\n",
    "        quantiles = df.stat.approxQuantile(col, [0.25, 0.75], 0.01)\n",
    "        q1, q3 = quantiles[0], quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # 2. Definimos límites (Bigotes del Boxplot)\n",
    "        limite_inf = q1 - 1.5 * iqr\n",
    "        limite_sup = q3 + 1.5 * iqr\n",
    "        \n",
    "        # 3. Contamos cuántos se salen\n",
    "        outliers = df.filter((F.col(col) < limite_inf) | (F.col(col) > limite_sup))\n",
    "        num_outliers = outliers.count()\n",
    "        \n",
    "        if num_outliers > 0:\n",
    "            print(f\"\\n Columna: {col}\")\n",
    "            print(f\"   Rango Normal: [{limite_inf:.2f}  a  {limite_sup:.2f}]\")\n",
    "            print(f\"   Outliers detectados: {num_outliers} filas\")\n",
    "            \n",
    "            # Mostramos los valores más extremos para ver si son errores o VIPs\n",
    "            print(f\"   Ejemplos (Top Extremos):\")\n",
    "            outliers.select(col).orderBy(F.desc(col)).show(3)\n",
    "        else:\n",
    "            print(f\"\\n Columna: {col} -> Sin outliers estadísticos.\")\n",
    "\n",
    "# Definimos las columnas numéricas críticas para analizar\n",
    "cols_analisis = [\"TOTAL_INCOME\", \"AGE_IN_YEARS\", \"AMOUNT_PRODUCT\", \"CREDICT_CARD_BALANCE\"]\n",
    "\n",
    "# Ejecutamos\n",
    "auditar_outliers(df_master, cols_analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075ae99",
   "metadata": {},
   "source": [
    "### Conversión de variables categóricas a numéricas con Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "df962892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "FASE 4: PREPARACIÓN PARA MODELOS (STRING INDEXER)\n",
      "========================================\n",
      "Columnas categóricas detectadas para transformar (10):\n",
      "['NAME_PRODUCT_TYPE', 'GENDER', 'EDUCATION', 'MARITAL_STATUS', 'HOME_SITUATION', 'OWN_INSURANCE_CAR', 'OCCUPATION', 'HOME_OWNER', 'EMPLOYER_ORGANIZATION_TYPE', 'KPI_AGE_GROUP']\n",
      "DATASET FINAL LISTO PARA ENTRENAMIENTO\n",
      "\n",
      "Ejemplo de datos transformados:\n",
      "+----------------------+------------+--------------+-----------+------------+------------+-------------+--------------+-----------+-------+-----------+----------------+-----------------+------------------+---------------------+------------------+--------------+---------------------+------------------------+---------------------------+-----------------------+-------------------+---------------------+-----------------+-------------------+----------------+--------------------+-----------------+-------------------+-------------------+---------------+------------------+-----------------+---------------+--------------+-------------------+------------------+---------------+---------------------+----------+-------------+------------------+------------------+---------------------+--------------+--------------+------------------------------+-----------------+\n",
      "|NON_COMPLIANT_CONTRACT|TOTAL_INCOME|AMOUNT_PRODUCT|INSTALLMENT|REGION_SCORE|AGE_IN_YEARS|JOB_SENIORITY|HOME_SENIORITY|LAST_UPDATE|CAR_AGE|FAMILY_SIZE|REACTIVE_SCORING|PROACTIVE_SCORING|BEHAVIORAL_SCORING|DAYS_LAST_INFO_CHANGE|NUMBER_OF_PRODUCTS|DIGITAL_CLIENT|NUM_PREVIOUS_LOAN_APP|LOAN_ANNUITY_PAYMENT_SUM|LOAN_APPLICATION_AMOUNT_SUM|LOAN_CREDIT_GRANTED_SUM|NUM_STATUS_ANNULLED|NUM_STATUS_AUTHORIZED|NUM_STATUS_DENIED|NUM_STATUS_NOT_USED|NUM_FLAG_INSURED|CREDICT_CARD_BALANCE|CREDIT_CARD_LIMIT|CREDIT_CARD_PAYMENT|NUMBER_DRAWINGS_ATM|NUMBER_DRAWINGS|NUMBER_INSTALMENTS|KPI_DAYS_LAST_MOV|KPI_TOTAL_SPEND|KPI_DEBT_RATIO|KPI_LOAN_VOLATILITY|KPI_APPROVAL_RATIO|KPI_DENIAL_RATE|NAME_PRODUCT_TYPE_IDX|GENDER_IDX|EDUCATION_IDX|MARITAL_STATUS_IDX|HOME_SITUATION_IDX|OWN_INSURANCE_CAR_IDX|OCCUPATION_IDX|HOME_OWNER_IDX|EMPLOYER_ORGANIZATION_TYPE_IDX|KPI_AGE_GROUP_IDX|\n",
      "+----------------------+------------+--------------+-----------+------------+------------+-------------+--------------+-----------+-------+-----------+----------------+-----------------+------------------+---------------------+------------------+--------------+---------------------+------------------------+---------------------------+-----------------------+-------------------+---------------------+-----------------+-------------------+----------------+--------------------+-----------------+-------------------+-------------------+---------------+------------------+-----------------+---------------+--------------+-------------------+------------------+---------------+---------------------+----------+-------------+------------------+------------------+---------------------+--------------+--------------+------------------------------+-----------------+\n",
      "|                     0|      1350.0|       6495.88|     276.75|      0.0197|          32|         3425|          4497|     4268.0|     12|          2|          0.1824|           0.5632|            0.5389|               2113.0|                 5|             1|                    5|                  467.66|                    4210.81|                4248.18|                0.0|                  5.0|              0.0|                0.0|             3.0|                 0.0|              0.0|                0.0|                  0|              0|                 0|             9999|            0.0|           0.0| 2128.2999999999997|              1.01|            0.0|                  0.0|       1.0|          3.0|               0.0|               1.0|                  1.0|           0.0|           0.0|                           5.0|              0.0|\n",
      "|                     0|       918.0|        8100.0|     261.31|      0.0085|          39|         1076|          3805|     4844.0|     -1|          2|            -1.0|           0.5197|            0.6864|               1083.0|                 1|             0|                    2|                   90.25|                     809.95|                 895.48|                1.0|                  1.0|              0.0|                0.0|             0.0|                 0.0|              0.0|                0.0|                  0|              0|                 0|             9999|            0.0|           0.0|             895.48|               1.1|            0.0|                  0.0|       0.0|          0.0|               0.0|               0.0|                  0.0|           0.0|           1.0|                           4.0|              0.0|\n",
      "|                     0|      1350.0|        3234.6|     298.24|      0.0051|          34|          390|          6610|     4362.0|     -1|          1|            -1.0|           0.2644|            0.5245|                  0.0|                 1|             0|                    3|                  256.53|                    1704.24|                1624.54|                0.0|                  3.0|              0.0|                0.0|             1.0|                 0.0|           1620.0|                0.0|                  0|              0|                 0|                0|            0.0|           0.0| 378.86000000000007|              0.95|            0.0|                  0.0|       1.0|          0.0|               1.0|               0.0|                  0.0|           0.0|           0.0|                          27.0|              0.0|\n",
      "|                     0|      1620.0|       4890.24|     386.37|      0.0462|          40|         1681|            56|     4992.0|     -1|          1|          0.6075|           0.6994|            0.6987|               1327.0|                 1|             0|                    2|                  152.56|                    1989.85|                2105.03|                0.0|                  2.0|              0.0|                0.0|             1.0|                 0.0|           1620.0|                0.0|                  0|              0|                54|               31|            0.0|           0.0| 306.93000000000006|              1.06|            0.0|                  0.0|       0.0|          0.0|               1.0|               0.0|                  0.0|           0.0|           1.0|                           9.0|              0.0|\n",
      "|                     0|      2430.0|       5004.29|      242.3|      0.0594|          61|          597|          8937|     5764.0|     -1|          2|            -1.0|           0.3142|            0.2314|                  0.0|                 1|             0|                    1|                   108.0|                        0.0|                 2160.0|                0.0|                  1.0|              0.0|                0.0|             1.0|                 0.0|              0.0|               2.83|                  0|              0|                13|                0|            0.0|           0.0|                0.0|            2160.0|            0.0|                  0.0|       0.0|          0.0|               0.0|               0.0|                  0.0|           0.0|           0.0|                          29.0|              1.0|\n",
      "+----------------------+------------+--------------+-----------+------------+------------+-------------+--------------+-----------+-------+-----------+----------------+-----------------+------------------+---------------------+------------------+--------------+---------------------+------------------------+---------------------------+-----------------------+-------------------+---------------------+-----------------+-------------------+----------------+--------------------+-----------------+-------------------+-------------------+---------------+------------------+-----------------+---------------+--------------+-------------------+------------------+---------------+---------------------+----------+-------------+------------------+------------------+---------------------+--------------+--------------+------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Convirtiendo a formato local (Pandas)...\n",
      "\n",
      " Archivo único guardado en: /home/jovyan/work/data/curated/Master_Model_FinPlus.parquet.\n",
      "root\n",
      " |-- NON_COMPLIANT_CONTRACT: integer (nullable = true)\n",
      " |-- TOTAL_INCOME: double (nullable = true)\n",
      " |-- AMOUNT_PRODUCT: double (nullable = true)\n",
      " |-- INSTALLMENT: double (nullable = true)\n",
      " |-- REGION_SCORE: double (nullable = true)\n",
      " |-- AGE_IN_YEARS: integer (nullable = true)\n",
      " |-- JOB_SENIORITY: integer (nullable = true)\n",
      " |-- HOME_SENIORITY: integer (nullable = true)\n",
      " |-- LAST_UPDATE: double (nullable = true)\n",
      " |-- CAR_AGE: integer (nullable = true)\n",
      " |-- FAMILY_SIZE: integer (nullable = true)\n",
      " |-- REACTIVE_SCORING: double (nullable = true)\n",
      " |-- PROACTIVE_SCORING: double (nullable = true)\n",
      " |-- BEHAVIORAL_SCORING: double (nullable = true)\n",
      " |-- DAYS_LAST_INFO_CHANGE: double (nullable = false)\n",
      " |-- NUMBER_OF_PRODUCTS: integer (nullable = true)\n",
      " |-- DIGITAL_CLIENT: integer (nullable = true)\n",
      " |-- NUM_PREVIOUS_LOAN_APP: integer (nullable = true)\n",
      " |-- LOAN_ANNUITY_PAYMENT_SUM: double (nullable = false)\n",
      " |-- LOAN_APPLICATION_AMOUNT_SUM: double (nullable = true)\n",
      " |-- LOAN_CREDIT_GRANTED_SUM: double (nullable = true)\n",
      " |-- NUM_STATUS_ANNULLED: double (nullable = false)\n",
      " |-- NUM_STATUS_AUTHORIZED: double (nullable = false)\n",
      " |-- NUM_STATUS_DENIED: double (nullable = false)\n",
      " |-- NUM_STATUS_NOT_USED: double (nullable = false)\n",
      " |-- NUM_FLAG_INSURED: double (nullable = false)\n",
      " |-- CREDICT_CARD_BALANCE: double (nullable = true)\n",
      " |-- CREDIT_CARD_LIMIT: double (nullable = true)\n",
      " |-- CREDIT_CARD_PAYMENT: double (nullable = false)\n",
      " |-- NUMBER_DRAWINGS_ATM: integer (nullable = true)\n",
      " |-- NUMBER_DRAWINGS: integer (nullable = true)\n",
      " |-- NUMBER_INSTALMENTS: integer (nullable = true)\n",
      " |-- KPI_DAYS_LAST_MOV: integer (nullable = true)\n",
      " |-- KPI_TOTAL_SPEND: double (nullable = false)\n",
      " |-- KPI_DEBT_RATIO: double (nullable = true)\n",
      " |-- KPI_LOAN_VOLATILITY: double (nullable = false)\n",
      " |-- KPI_APPROVAL_RATIO: double (nullable = true)\n",
      " |-- KPI_DENIAL_RATE: double (nullable = true)\n",
      " |-- NAME_PRODUCT_TYPE_IDX: double (nullable = false)\n",
      " |-- GENDER_IDX: double (nullable = false)\n",
      " |-- EDUCATION_IDX: double (nullable = false)\n",
      " |-- MARITAL_STATUS_IDX: double (nullable = false)\n",
      " |-- HOME_SITUATION_IDX: double (nullable = false)\n",
      " |-- OWN_INSURANCE_CAR_IDX: double (nullable = false)\n",
      " |-- OCCUPATION_IDX: double (nullable = false)\n",
      " |-- HOME_OWNER_IDX: double (nullable = false)\n",
      " |-- EMPLOYER_ORGANIZATION_TYPE_IDX: double (nullable = false)\n",
      " |-- KPI_AGE_GROUP_IDX: double (nullable = false)\n",
      "\n",
      "INSPECCIÓN DE: MASTER MODEL\n",
      "\n",
      "Dimensiones: 162977 filas x 48 columnas\n",
      "\n",
      "--- 1. Conteo de Nulos ---\n",
      "-RECORD 0-----------------------------\n",
      " NON_COMPLIANT_CONTRACT         | 0   \n",
      " TOTAL_INCOME                   | 0   \n",
      " AMOUNT_PRODUCT                 | 0   \n",
      " INSTALLMENT                    | 0   \n",
      " REGION_SCORE                   | 0   \n",
      " AGE_IN_YEARS                   | 0   \n",
      " JOB_SENIORITY                  | 0   \n",
      " HOME_SENIORITY                 | 0   \n",
      " LAST_UPDATE                    | 0   \n",
      " CAR_AGE                        | 0   \n",
      " FAMILY_SIZE                    | 0   \n",
      " REACTIVE_SCORING               | 0   \n",
      " PROACTIVE_SCORING              | 0   \n",
      " BEHAVIORAL_SCORING             | 0   \n",
      " DAYS_LAST_INFO_CHANGE          | 0   \n",
      " NUMBER_OF_PRODUCTS             | 0   \n",
      " DIGITAL_CLIENT                 | 0   \n",
      " NUM_PREVIOUS_LOAN_APP          | 0   \n",
      " LOAN_ANNUITY_PAYMENT_SUM       | 0   \n",
      " LOAN_APPLICATION_AMOUNT_SUM    | 0   \n",
      " LOAN_CREDIT_GRANTED_SUM        | 0   \n",
      " NUM_STATUS_ANNULLED            | 0   \n",
      " NUM_STATUS_AUTHORIZED          | 0   \n",
      " NUM_STATUS_DENIED              | 0   \n",
      " NUM_STATUS_NOT_USED            | 0   \n",
      " NUM_FLAG_INSURED               | 0   \n",
      " CREDICT_CARD_BALANCE           | 0   \n",
      " CREDIT_CARD_LIMIT              | 0   \n",
      " CREDIT_CARD_PAYMENT            | 0   \n",
      " NUMBER_DRAWINGS_ATM            | 0   \n",
      " NUMBER_DRAWINGS                | 0   \n",
      " NUMBER_INSTALMENTS             | 0   \n",
      " KPI_DAYS_LAST_MOV              | 0   \n",
      " KPI_TOTAL_SPEND                | 0   \n",
      " KPI_DEBT_RATIO                 | 0   \n",
      " KPI_LOAN_VOLATILITY            | 0   \n",
      " KPI_APPROVAL_RATIO             | 0   \n",
      " KPI_DENIAL_RATE                | 0   \n",
      " NAME_PRODUCT_TYPE_IDX          | 0   \n",
      " GENDER_IDX                     | 0   \n",
      " EDUCATION_IDX                  | 0   \n",
      " MARITAL_STATUS_IDX             | 0   \n",
      " HOME_SITUATION_IDX             | 0   \n",
      " OWN_INSURANCE_CAR_IDX          | 0   \n",
      " OCCUPATION_IDX                 | 0   \n",
      " HOME_OWNER_IDX                 | 0   \n",
      " EMPLOYER_ORGANIZATION_TYPE_IDX | 0   \n",
      " KPI_AGE_GROUP_IDX              | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8. TRANSFORMACIÓN FINAL: LABEL ENCODING PARA ML\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FASE 4: PREPARACIÓN PARA MODELOS (STRING INDEXER)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Detectar columnas de texto automáticamente\n",
    "# Definimos la \"Lista Negra\" de columnas que NO son características (IDs, Fechas, etc.)\n",
    "cols_excluir = [\"CLIENT_ID\", \"CONTRACT_ID\", \"DATE\", \"date\"] \n",
    "\n",
    "# Buscamos todas las columnas de tipo String que no estén en la lista de exclusión\n",
    "categ_cols = [f.name for f in df_master.schema.fields \n",
    "              if isinstance(f.dataType, T.StringType) and f.name not in cols_excluir]\n",
    "\n",
    "print(f\"Columnas categóricas detectadas para transformar ({len(categ_cols)}):\")\n",
    "print(categ_cols)\n",
    "\n",
    "# 2. Configurar el StringIndexer (Label Encoding)\n",
    "# - handleInvalid=\"keep\": Si en el futuro aparece una categoría nueva no vista, crea un índice extra en vez de dar error.\n",
    "# - outputCol: Crea nuevas columnas terminadas en \"_IDX\".\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_IDX\", handleInvalid=\"keep\") for col in categ_cols]\n",
    "\n",
    "# 3. Ejecutar la transformación (Pipeline)\n",
    "# Usamos un Pipeline para aplicar todos los indexers de golpe de forma eficiente\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "model = pipeline.fit(df_master)\n",
    "df_model_ready = model.transform(df_master)\n",
    "\n",
    "# 4. Selección Final: Solo columnas numéricas\n",
    "# Nos quedamos con las numéricas originales + los nuevos índices (_IDX).\n",
    "# Descartamos las columnas de texto originales y los IDs.\n",
    "\n",
    "# Lista de columnas numéricas originales (Double, Integer, Long)\n",
    "numeric_cols_orig = [f.name for f in df_master.schema.fields \n",
    "                     if isinstance(f.dataType, (T.DoubleType, T.IntegerType, T.LongType))]\n",
    "\n",
    "# Lista de las nuevas columnas indexadas\n",
    "idx_cols = [f\"{col}_IDX\" for col in categ_cols]\n",
    "\n",
    "# Ensamblamos la lista final\n",
    "cols_definitivas = numeric_cols_orig + idx_cols\n",
    "df_final_numeric = df_model_ready.select(cols_definitivas)\n",
    "\n",
    "# 5. RESULTADO Y GUARDADO\n",
    "print(\"DATASET FINAL LISTO PARA ENTRENAMIENTO\")\n",
    "\n",
    "# Mostrar un ejemplo de cómo quedan los datos\n",
    "print(\"\\nEjemplo de datos transformados:\")\n",
    "df_final_numeric.show(5)\n",
    "\n",
    "# --- E. GUARDADO EN PARQUET ÚNICO ---\n",
    "# 1. Convertimos de Spark a Pandas\n",
    "print(\"Convirtiendo a formato local (Pandas)...\")\n",
    "df_pandas2 = df_final_numeric.toPandas()\n",
    "\n",
    "# 2. Definimos nombre y ruta\n",
    "nombre_archivo = \"Master_Model_FinPlus.parquet\"\n",
    "ruta_completa = OUTPUT_PATH + nombre_archivo\n",
    "\n",
    "# 3. Guardamos el archivo físico\n",
    "# index=False evita que se guarde el número de fila como una columna extra\n",
    "df_pandas2.to_parquet(ruta_completa, index=False)\n",
    "\n",
    "print(f\"\\n Archivo único guardado en: {ruta_completa}.\")\n",
    "\n",
    "df_final_numeric.printSchema()\n",
    "inspeccionar_datos(df_final_numeric, \"MASTER MODEL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b2f9f",
   "metadata": {},
   "source": [
    "# INDICADORES Y ANÁLISIS DE COMPORTAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053ba90",
   "metadata": {},
   "source": [
    "### ACTIVIDAD CLIENTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0ab92",
   "metadata": {},
   "source": [
    "- Cálculo de métricas de actividad:\n",
    "    - Recencia (R)\n",
    "\n",
    "    - Frecuencia (F)\n",
    "\n",
    "    - Intensidad (I)\n",
    "\n",
    "- Ventanas de actividad (30/90/180 días)\n",
    "\n",
    "- Meses activos\n",
    "\n",
    "- Clasificación de actividad (Alta / Media / Baja)\n",
    "\n",
    "- Interpretación clara de negocio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dbc96c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. CARGAR DATOS YA PROCESADOS\n",
    "# ==========================================\n",
    "DATA_PATH = \"/home/jovyan/work/data/\"\n",
    "\n",
    "beh = spark.read.parquet(DATA_PATH + \"BEHAVIOURAL.parquet\")\n",
    "df_master = spark.read.parquet(DATA_PATH + \"curated/Master_FinPlus.parquet\")\n",
    "\n",
    "beh = beh.withColumn(\"DATE\", F.to_date(\"DATE\", \"yyyy-MM-dd\"))\n",
    "max_date = beh.agg(F.max(\"DATE\")).first()[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9dff808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RECENCIA (R) ================\n",
      "\n",
      "Interpretación: número de días desde la última actividad del cliente.\n",
      "+-------+------------------+\n",
      "|summary|      RECENCY_DAYS|\n",
      "+-------+------------------+\n",
      "|  count|             46046|\n",
      "|   mean|15.444794336098685|\n",
      "| stddev| 21.54619528266414|\n",
      "|    min|                 0|\n",
      "|    max|                93|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "--- Top 10 clientes más recientes ---\n",
      "+------------+------------------+------------+\n",
      "|CLIENT_ID   |last_activity_date|RECENCY_DAYS|\n",
      "+------------+------------------+------------+\n",
      "|ES182303796D|2021-12-31        |0           |\n",
      "|ES182245752Y|2021-12-31        |0           |\n",
      "|ES182293250V|2021-12-31        |0           |\n",
      "|ES182245476M|2021-12-31        |0           |\n",
      "|ES182112694Y|2021-12-31        |0           |\n",
      "|ES182189508S|2021-12-31        |0           |\n",
      "|ES182433571C|2021-12-31        |0           |\n",
      "|ES182232062V|2021-12-31        |0           |\n",
      "|ES182378189N|2021-12-31        |0           |\n",
      "|ES182279031S|2021-12-31        |0           |\n",
      "+------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- Top 10 clientes más abandonados ---\n",
      "+------------+------------------+------------+\n",
      "|CLIENT_ID   |last_activity_date|RECENCY_DAYS|\n",
      "+------------+------------------+------------+\n",
      "|ES182406226A|2021-09-29        |93          |\n",
      "|ES182205817D|2021-09-29        |93          |\n",
      "|ES182211003Y|2021-09-29        |93          |\n",
      "|ES182451631D|2021-09-29        |93          |\n",
      "|ES182406265H|2021-09-29        |93          |\n",
      "|ES182405527S|2021-09-29        |93          |\n",
      "|ES182211280N|2021-09-29        |93          |\n",
      "|ES182179190D|2021-09-29        |93          |\n",
      "|ES182306422P|2021-09-29        |93          |\n",
      "|ES182109223R|2021-09-29        |93          |\n",
      "+------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. MÉTRICAS DE ACTIVIDAD\n",
    "# ==========================================\n",
    "\n",
    "# --- RECENCIA ---\n",
    "recencia_df = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    F.max(\"DATE\").alias(\"last_activity_date\")\n",
    ").withColumn(\n",
    "    \"RECENCY_DAYS\", F.datediff(F.lit(max_date), F.col(\"last_activity_date\"))\n",
    ")\n",
    "\n",
    "print(\"\\n================ RECENCIA (R) ================\\n\")\n",
    "print(\"Interpretación: número de días desde la última actividad del cliente.\")\n",
    "recencia_df.describe(\"RECENCY_DAYS\").show()\n",
    "\n",
    "print(\"\\n--- Top 10 clientes más recientes ---\")\n",
    "recencia_df.orderBy(F.col(\"RECENCY_DAYS\").asc()).show(10, truncate=False)\n",
    "\n",
    "print(\"\\n--- Top 10 clientes más abandonados ---\")\n",
    "recencia_df.orderBy(F.col(\"RECENCY_DAYS\").desc()).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a845daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ FRECUENCIA (F) ================\n",
      "\n",
      "Interpretación: cuántos movimientos totales ha realizado el cliente.\n",
      "+-------+-----------------+\n",
      "|summary|  FREQUENCY_COUNT|\n",
      "+-------+-----------------+\n",
      "|  count|            46046|\n",
      "|   mean|37.45936672023628|\n",
      "| stddev|33.78619079695795|\n",
      "|    min|                1|\n",
      "|    max|              192|\n",
      "+-------+-----------------+\n",
      "\n",
      "\n",
      "--- Top 10 clientes con mayor frecuencia ---\n",
      "+------------+---------------+\n",
      "|CLIENT_ID   |FREQUENCY_COUNT|\n",
      "+------------+---------------+\n",
      "|ES182186401T|192            |\n",
      "|ES182128827G|129            |\n",
      "|ES182192917N|126            |\n",
      "|ES182283225D|122            |\n",
      "|ES182378495D|122            |\n",
      "|ES182155668A|121            |\n",
      "|ES182253915Y|121            |\n",
      "|ES182146380G|121            |\n",
      "|ES182267366X|120            |\n",
      "|ES182210848C|120            |\n",
      "+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FRECUENCIA ---\n",
    "frecuencia_df = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    F.count(\"*\").alias(\"FREQUENCY_COUNT\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ FRECUENCIA (F) ================\\n\")\n",
    "print(\"Interpretación: cuántos movimientos totales ha realizado el cliente.\")\n",
    "frecuencia_df.describe(\"FREQUENCY_COUNT\").show()\n",
    "\n",
    "print(\"\\n--- Top 10 clientes con mayor frecuencia ---\")\n",
    "frecuencia_df.orderBy(F.col(\"FREQUENCY_COUNT\").desc()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a386cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ INTENSIDAD (I) ================\n",
      "\n",
      "Interpretación: gasto promedio por movimiento del cliente.\n",
      "+-------+-------------------+\n",
      "|summary|INTENSITY_AVG_SPEND|\n",
      "+-------+-------------------+\n",
      "|  count|              46046|\n",
      "|   mean| 165.34456214658363|\n",
      "| stddev|  310.5056279798483|\n",
      "|    min|                0.0|\n",
      "|    max|  9629.483333333334|\n",
      "+-------+-------------------+\n",
      "\n",
      "\n",
      "--- Top 10 clientes con mayor intensidad ---\n",
      "+------------+-------------------+\n",
      "|CLIENT_ID   |INTENSITY_AVG_SPEND|\n",
      "+------------+-------------------+\n",
      "|ES182227107Z|9629.483333333334  |\n",
      "|ES182354235A|8642.13            |\n",
      "|ES182436756T|7395.070000000001  |\n",
      "|ES182156500S|5901.773846153846  |\n",
      "|ES182366160A|5899.592142857144  |\n",
      "|ES182390308Q|5731.825           |\n",
      "|ES182247132U|5427.0             |\n",
      "|ES182396618L|5346.0             |\n",
      "|ES182129030R|5245.332173913043  |\n",
      "|ES182250483A|5186.4158333333335 |\n",
      "+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- INTENSIDAD ---\n",
    "beh = beh.withColumn(\n",
    "    \"KPI_TOTAL_SPEND\",\n",
    "    F.coalesce(F.col(\"CREDIT_CARD_DRAWINGS_ATM\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"CREDIT_CARD_DRAWINGS_POS\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"CREDIT_CARD_DRAWINGS_OTHER\"), F.lit(0))\n",
    ")\n",
    "\n",
    "intensidad_df = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    F.avg(\"KPI_TOTAL_SPEND\").alias(\"INTENSITY_AVG_SPEND\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ INTENSIDAD (I) ================\\n\")\n",
    "print(\"Interpretación: gasto promedio por movimiento del cliente.\")\n",
    "intensidad_df.describe(\"INTENSITY_AVG_SPEND\").show()\n",
    "\n",
    "print(\"\\n--- Top 10 clientes con mayor intensidad ---\")\n",
    "intensidad_df.orderBy(F.col(\"INTENSITY_AVG_SPEND\").desc()).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af8361f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ ACTIVIDAD EN VENTANAS ================\n",
      "\n",
      "Interpretación: cuántas interacciones ha tenido el cliente en los últimos X días.\n",
      "\n",
      "--- Top 10 actividad últimos 30 días ---\n",
      "+------------+------------+------------+-------------+\n",
      "|CLIENT_ID   |ACTIVITY_30D|ACTIVITY_90D|ACTIVITY_180D|\n",
      "+------------+------------+------------+-------------+\n",
      "|ES182403907W|2           |6           |12           |\n",
      "|ES182334710B|2           |6           |12           |\n",
      "|ES182210848C|2           |6           |12           |\n",
      "|ES182347896Q|2           |6           |12           |\n",
      "|ES182243027C|2           |6           |11           |\n",
      "|ES182407401Q|2           |6           |12           |\n",
      "|ES182127891W|2           |5           |8            |\n",
      "|ES182150696Z|2           |6           |12           |\n",
      "|ES182100594F|2           |4           |7            |\n",
      "|ES182283225D|2           |6           |12           |\n",
      "+------------+------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--- Top 10 actividad últimos 90 días ---\n",
      "+------------+------------+------------+-------------+\n",
      "|CLIENT_ID   |ACTIVITY_30D|ACTIVITY_90D|ACTIVITY_180D|\n",
      "+------------+------------+------------+-------------+\n",
      "|ES182425933U|2           |6           |12           |\n",
      "|ES182251358N|2           |6           |10           |\n",
      "|ES182334710B|2           |6           |12           |\n",
      "|ES182403907W|2           |6           |12           |\n",
      "|ES182300727X|2           |6           |12           |\n",
      "|ES182347896Q|2           |6           |12           |\n",
      "|ES182139528A|2           |6           |12           |\n",
      "|ES182407401Q|2           |6           |12           |\n",
      "|ES182212183S|2           |6           |12           |\n",
      "|ES182150696Z|2           |6           |12           |\n",
      "+------------+------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--- Top 10 actividad últimos 180 días ---\n",
      "+------------+------------+------------+-------------+\n",
      "|CLIENT_ID   |ACTIVITY_30D|ACTIVITY_90D|ACTIVITY_180D|\n",
      "+------------+------------+------------+-------------+\n",
      "|ES182212183S|2           |6           |12           |\n",
      "|ES182291091W|2           |6           |12           |\n",
      "|ES182217001J|2           |6           |12           |\n",
      "|ES182403907W|2           |6           |12           |\n",
      "|ES182311871L|2           |6           |12           |\n",
      "|ES182347896Q|2           |6           |12           |\n",
      "|ES182211607S|2           |6           |12           |\n",
      "|ES182407401Q|2           |6           |12           |\n",
      "|ES182118190I|2           |6           |12           |\n",
      "|ES182425933U|2           |6           |12           |\n",
      "+------------+------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. ACTIVIDAD 30 / 90 / 180 DÍAS\n",
    "# ==========================================\n",
    "beh_windows = beh.withColumn(\n",
    "    \"DAYS_FROM_REF\", F.datediff(F.lit(max_date), F.col(\"DATE\"))\n",
    ")\n",
    "\n",
    "ventanas_df = beh_windows.groupBy(\"CLIENT_ID\").agg(\n",
    "    F.sum(F.when(F.col(\"DAYS_FROM_REF\") <= 30, 1).otherwise(0)).alias(\"ACTIVITY_30D\"),\n",
    "    F.sum(F.when(F.col(\"DAYS_FROM_REF\") <= 90, 1).otherwise(0)).alias(\"ACTIVITY_90D\"),\n",
    "    F.sum(F.when(F.col(\"DAYS_FROM_REF\") <= 180, 1).otherwise(0)).alias(\"ACTIVITY_180D\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ ACTIVIDAD EN VENTANAS ================\\n\")\n",
    "print(\"Interpretación: cuántas interacciones ha tenido el cliente en los últimos X días.\\n\")\n",
    "\n",
    "print(\"--- Top 10 actividad últimos 30 días ---\")\n",
    "ventanas_df.orderBy(F.col(\"ACTIVITY_30D\").desc()).show(10, truncate=False)\n",
    "\n",
    "print(\"--- Top 10 actividad últimos 90 días ---\")\n",
    "ventanas_df.orderBy(F.col(\"ACTIVITY_90D\").desc()).show(10, truncate=False)\n",
    "\n",
    "print(\"--- Top 10 actividad últimos 180 días ---\")\n",
    "ventanas_df.orderBy(F.col(\"ACTIVITY_180D\").desc()).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fded75fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ MESES ACTIVOS ================\n",
      "\n",
      "+------------+-------------+\n",
      "|CLIENT_ID   |ACTIVE_MONTHS|\n",
      "+------------+-------------+\n",
      "|ES182230930K|96           |\n",
      "|ES182173222Z|96           |\n",
      "|ES182384509S|96           |\n",
      "|ES182167286W|96           |\n",
      "|ES182123143P|96           |\n",
      "|ES182233737B|96           |\n",
      "|ES182254666P|96           |\n",
      "|ES182258895P|96           |\n",
      "|ES182243311L|96           |\n",
      "|ES182350305O|96           |\n",
      "+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. MESES ACTIVOS\n",
    "# ==========================================\n",
    "beh_month = beh.withColumn(\n",
    "    \"YEAR_MONTH\", F.date_format(\"DATE\", \"yyyy-MM\")\n",
    ")\n",
    "\n",
    "meses_activos_df = beh_month.groupBy(\"CLIENT_ID\").agg(\n",
    "    F.countDistinct(\"YEAR_MONTH\").alias(\"ACTIVE_MONTHS\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ MESES ACTIVOS ================\\n\")\n",
    "meses_activos_df.orderBy(F.col(\"ACTIVE_MONTHS\").desc()).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8aaf83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. COMBINAR TODAS LAS MÉTRICAS\n",
    "# ==========================================\n",
    "activity_metrics = (\n",
    "    recencia_df\n",
    "    .join(frecuencia_df, \"CLIENT_ID\", \"left\")\n",
    "    .join(intensidad_df, \"CLIENT_ID\", \"left\")\n",
    "    .join(ventanas_df, \"CLIENT_ID\", \"left\")\n",
    "    .join(meses_activos_df, \"CLIENT_ID\", \"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d67db52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ SEGMENTACIÓN FINAL ================\n",
      "\n",
      "+----------------+-----+\n",
      "|ACTIVITY_SEGMENT|count|\n",
      "+----------------+-----+\n",
      "|            Alta|26002|\n",
      "|           Media|18448|\n",
      "|            Baja| 1596|\n",
      "+----------------+-----+\n",
      "\n",
      "\n",
      "--- Muestra de clientes segmentados ---\n",
      "+------------+------------+---------------+-------------------+----------------+\n",
      "|CLIENT_ID   |RECENCY_DAYS|FREQUENCY_COUNT|INTENSITY_AVG_SPEND|ACTIVITY_SEGMENT|\n",
      "+------------+------------+---------------+-------------------+----------------+\n",
      "|ES182222478Q|0           |19             |588.2236842105264  |Alta            |\n",
      "|ES182325278Y|0           |9              |625.4711111111111  |Alta            |\n",
      "|ES182133642C|0           |85             |31.129411764705882 |Alta            |\n",
      "|ES182245476M|0           |10             |0.0                |Alta            |\n",
      "|ES182413873B|0           |91             |106.26186813186811 |Alta            |\n",
      "|ES182245752Y|0           |26             |0.0                |Alta            |\n",
      "|ES182291686Y|0           |91             |26.10989010989011  |Alta            |\n",
      "|ES182232062V|0           |9              |699.2444444444444  |Alta            |\n",
      "|ES182231902V|0           |96             |0.0                |Alta            |\n",
      "|ES182279031S|0           |96             |23.79375           |Alta            |\n",
      "|ES182176939F|0           |28             |0.0                |Alta            |\n",
      "|ES182196536J|0           |7              |594.6414285714285  |Alta            |\n",
      "|ES182145473H|0           |15             |273.96733333333333 |Alta            |\n",
      "|ES182303796D|0           |93             |60.472043010752685 |Alta            |\n",
      "|ES182293250V|0           |47             |330.3663829787234  |Alta            |\n",
      "|ES182112694Y|0           |9              |0.0                |Alta            |\n",
      "|ES182297392F|0           |84             |11.445714285714287 |Alta            |\n",
      "|ES182378189N|0           |96             |13.949375000000002 |Alta            |\n",
      "|ES182186318H|0           |6              |354.57666666666665 |Alta            |\n",
      "|ES182360017K|0           |96             |6.1875             |Alta            |\n",
      "+------------+------------+---------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6. SEGMENTACIÓN DE ACTIVIDAD\n",
    "# ==========================================\n",
    "activity_metrics = activity_metrics.withColumn(\n",
    "    \"ACTIVITY_SEGMENT\",\n",
    "    F.when((F.col(\"RECENCY_DAYS\") <= 30) & (F.col(\"FREQUENCY_COUNT\") > 5), \"Alta\")\n",
    "     .when((F.col(\"RECENCY_DAYS\") <= 90) & (F.col(\"FREQUENCY_COUNT\") > 2), \"Media\")\n",
    "     .otherwise(\"Baja\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ SEGMENTACIÓN FINAL ================\\n\")\n",
    "activity_metrics.groupBy(\"ACTIVITY_SEGMENT\").count().show()\n",
    "\n",
    "print(\"\\n--- Muestra de clientes segmentados ---\")\n",
    "activity_metrics.select(\n",
    "    \"CLIENT_ID\", \"RECENCY_DAYS\", \"FREQUENCY_COUNT\", \n",
    "    \"INTENSITY_AVG_SPEND\", \"ACTIVITY_SEGMENT\"\n",
    ").orderBy(\"ACTIVITY_SEGMENT\").show(20, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "728f9610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo guardado correctamente en: Master_FinPlus_Activity.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7. UNIR A MASTER FINAL\n",
    "# ==========================================\n",
    "df_final = df_master.join(activity_metrics, \"CLIENT_ID\", \"left\")\n",
    "df_final.write.mode(\"overwrite\").parquet(DATA_PATH + \"Master_FinPlus_Activity.parquet\")\n",
    "\n",
    "print(\"\\nArchivo guardado correctamente en: Master_FinPlus_Activity.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ee14d",
   "metadata": {},
   "source": [
    "### VALOR ECONÓMICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f97a2",
   "metadata": {},
   "source": [
    "- ECONOMIC VALUE SCORE (E-SCORE)\n",
    "    - Total gastado\n",
    "\n",
    "    - Volumen de compras\n",
    "\n",
    "    - Número de compras\n",
    "\n",
    "    - Rentabilidad generada\n",
    "\n",
    "- MÉTRICAS POR CLIENTE\n",
    "    - Gasto total\n",
    "\n",
    "    - Ticket medio\n",
    "\n",
    "    - Varianza del gasto\n",
    "\n",
    "    - Compra máxima\n",
    "\n",
    "    - Compra mínima\n",
    "\n",
    "    - Ratio de recurrecia económica\n",
    "\n",
    "    - Índices de \"customer value\" (cuartiles o percentiles)\n",
    "\n",
    "- SEGMENTACIÓN\n",
    "    - Alto valor\n",
    "\n",
    "    - Medio\n",
    "\n",
    "    - Bajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5e19561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 1. MÉTRICAS ECONÓMICAS BÁSICAS ================\n",
      "\n",
      "Métricas: Gasto Total, Ticket Medio, Varianza, Max/Min Compra, Nº Transacciones.\n",
      "✔ Resumen descriptivo del gasto total (TOTAL_SPEND)\n",
      "+-------+-----------------+\n",
      "|summary|      TOTAL_SPEND|\n",
      "+-------+-----------------+\n",
      "|  count|            46046|\n",
      "|   mean|3348.860903227201|\n",
      "| stddev|5342.655025746742|\n",
      "|    min|              0.0|\n",
      "|    max|        192187.19|\n",
      "+-------+-----------------+\n",
      "\n",
      "\n",
      "✔ Top 10 clientes con mayor gasto total:\n",
      "+------------+-----------------+------------------+------------------+------------+------------+----------------+\n",
      "|CLIENT_ID   |TOTAL_SPEND      |AVG_TICKET        |SPEND_STDDEV      |MAX_PURCHASE|MIN_PURCHASE|NUM_TRANSACTIONS|\n",
      "+------------+-----------------+------------------+------------------+------------+------------+----------------+\n",
      "|ES182348429U|192187.19        |5057.557631578948 |1830.513040213463 |8507.65     |186.6       |38              |\n",
      "|ES182227107Z|144442.25        |9629.483333333334 |5771.064596772749 |24720.36    |351.8       |15              |\n",
      "|ES182129030R|120642.64        |5245.332173913043 |4613.432042747033 |14501.23    |0.0         |23              |\n",
      "|ES182117163D|100618.2         |2579.953846153846 |1376.4885832868463|4050.0      |0.0         |39              |\n",
      "|ES182403153O|99603.0          |2845.8            |1496.3801380822517|5184.0      |0.0         |35              |\n",
      "|ES182262674U|91530.0          |2773.6363636363635|3781.875508743825 |10800.0     |0.0         |33              |\n",
      "|ES182446227G|87831.59000000003|3659.6495833333342|2715.4788727644805|9228.72     |0.0         |24              |\n",
      "|ES182116963C|84592.95         |2819.765          |1183.1869435197118|5723.7      |0.0         |30              |\n",
      "|ES182132454N|84581.42000000001|972.2002298850576 |1596.4626979443817|6251.25     |0.0         |87              |\n",
      "|ES182423117N|84021.48999999999|3231.595769230769 |1901.8653199491769|7550.76     |0.0         |26              |\n",
      "+------------+-----------------+------------------+------------------+------------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "✔ Ticket medio (Top 10 más altos):\n",
      "+------------+------------------+------------------+------------------+------------+------------+----------------+\n",
      "|CLIENT_ID   |TOTAL_SPEND       |AVG_TICKET        |SPEND_STDDEV      |MAX_PURCHASE|MIN_PURCHASE|NUM_TRANSACTIONS|\n",
      "+------------+------------------+------------------+------------------+------------+------------+----------------+\n",
      "|ES182227107Z|144442.25         |9629.483333333334 |5771.064596772749 |24720.36    |351.8       |15              |\n",
      "|ES182354235A|8642.13           |8642.13           |NULL              |8642.13     |8642.13     |1               |\n",
      "|ES182436756T|51765.490000000005|7395.070000000001 |4440.58433640589  |15406.69    |2682.66     |7               |\n",
      "|ES182156500S|76723.06          |5901.773846153846 |4553.407369821231 |13305.55    |0.0         |13              |\n",
      "|ES182366160A|82594.29000000001 |5899.592142857144 |2335.9468982346275|9620.77     |229.36      |14              |\n",
      "|ES182390308Q|11463.65          |5731.825          |5563.763641749171 |9666.0      |1797.65     |2               |\n",
      "|ES182247132U|10854.0           |5427.0            |7674.937002998787 |10854.0     |0.0         |2               |\n",
      "|ES182396618L|5346.0            |5346.0            |NULL              |5346.0      |5346.0      |1               |\n",
      "|ES182129030R|120642.64         |5245.332173913043 |4613.432042747033 |14501.23    |0.0         |23              |\n",
      "|ES182250483A|62236.99          |5186.4158333333335|2353.9103842270565|8196.34     |810.0       |12              |\n",
      "+------------+------------------+------------------+------------------+------------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "✔ Distribución de número de transacciones:\n",
      "+-------+-----------------+\n",
      "|summary| NUM_TRANSACTIONS|\n",
      "+-------+-----------------+\n",
      "|  count|            46046|\n",
      "|   mean|37.45936672023628|\n",
      "| stddev|33.78619079695795|\n",
      "|    min|                1|\n",
      "|    max|              192|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. MÉTRICAS ECONÓMICAS BÁSICAS POR CLIENTE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "econ = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    # total gastado, volumen de compras\n",
    "    F.sum(\"KPI_TOTAL_SPEND\").alias(\"TOTAL_SPEND\"), \n",
    "    \n",
    "    # rentabilidad generada (usamos gasto promedio como proxy de valor por transacción)\n",
    "    F.avg(\"KPI_TOTAL_SPEND\").alias(\"AVG_TICKET\"), # Ticket medio\n",
    "    \n",
    "    # varianza del gasto (usamos stddev como medida de dispersión/volatilidad)\n",
    "    F.stddev(\"KPI_TOTAL_SPEND\").alias(\"SPEND_STDDEV\"),\n",
    "    \n",
    "    # compra máxima\n",
    "    F.max(\"KPI_TOTAL_SPEND\").alias(\"MAX_PURCHASE\"),\n",
    "    \n",
    "    # compra mínima\n",
    "    F.min(\"KPI_TOTAL_SPEND\").alias(\"MIN_PURCHASE\"),\n",
    "    \n",
    "    # numero de compras\n",
    "    F.count(\"*\").alias(\"NUM_TRANSACTIONS\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ 1. MÉTRICAS ECONÓMICAS BÁSICAS ================\\n\")\n",
    "print(\"Métricas: Gasto Total, Ticket Medio, Varianza, Max/Min Compra, Nº Transacciones.\")\n",
    "\n",
    "print(\"✔ Resumen descriptivo del gasto total (TOTAL_SPEND)\")\n",
    "econ.select(\"TOTAL_SPEND\").describe().show()\n",
    "\n",
    "print(\"\\n✔ Top 10 clientes con mayor gasto total:\")\n",
    "econ.orderBy(F.col(\"TOTAL_SPEND\").desc()).show(10, truncate=False)\n",
    "\n",
    "print(\"\\n✔ Ticket medio (Top 10 más altos):\")\n",
    "econ.orderBy(F.col(\"AVG_TICKET\").desc()).show(10, truncate=False)\n",
    "\n",
    "print(\"\\n✔ Distribución de número de transacciones:\")\n",
    "econ.select(\"NUM_TRANSACTIONS\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3fe186e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 2. RATIO DE RECURRENCIA ECONÓMICA ================\n",
      "\n",
      "Ratio: Gasto Total / (Nº Transacciones + 1).\n",
      "+------------+------------------+----------------+----------------------+\n",
      "|CLIENT_ID   |TOTAL_SPEND       |NUM_TRANSACTIONS|SPEND_RECURRENCE_RATIO|\n",
      "+------------+------------------+----------------+----------------------+\n",
      "|ES182227107Z|144442.25         |15              |9027.64               |\n",
      "|ES182436756T|51765.490000000005|7               |6470.69               |\n",
      "|ES182366160A|82594.29000000001 |14              |5506.29               |\n",
      "|ES182156500S|76723.06          |13              |5480.22               |\n",
      "|ES182129030R|120642.64         |23              |5026.78               |\n",
      "|ES182348429U|192187.19         |38              |4927.88               |\n",
      "|ES182446412N|72112.26          |14              |4807.48               |\n",
      "|ES182250483A|62236.99          |12              |4787.46               |\n",
      "|ES182358570I|79456.63          |16              |4673.92               |\n",
      "|ES182354235A|8642.13           |1               |4321.07               |\n",
      "+------------+------------------+----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. RATIO DE RECURRENCIA ECONÓMICA\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "econ = econ.withColumn(\n",
    "    \"SPEND_RECURRENCE_RATIO\",\n",
    "    # Ratio: Gasto Total / (Nº Transacciones + 1). Cuanto más alto, mayor el gasto promedio por evento.\n",
    "    F.round(F.col(\"TOTAL_SPEND\") / (F.col(\"NUM_TRANSACTIONS\") + 1), 2)\n",
    ")\n",
    "\n",
    "print(\"\\n================ 2. RATIO DE RECURRENCIA ECONÓMICA ================\\n\")\n",
    "print(\"Ratio: Gasto Total / (Nº Transacciones + 1).\")\n",
    "econ.select(\"CLIENT_ID\", \"TOTAL_SPEND\", \"NUM_TRANSACTIONS\", \"SPEND_RECURRENCE_RATIO\") \\\n",
    "    .orderBy(F.col(\"SPEND_RECURRENCE_RATIO\").desc()) \\\n",
    "    .show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5bb5d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 3. SEGMENTACIÓN DE VALOR ECONÓMICO (E-SCORE) ================\n",
      "\n",
      "Segmentación (Alto, Medio, Bajo) basada en los tertiles de Gasto Total (TOTAL_SPEND).\n",
      "\n",
      "✔ Segmentación económica completada:\n",
      "+--------------------+-----+\n",
      "|ECONOMIC_VALUE_CLASS|count|\n",
      "+--------------------+-----+\n",
      "|          ALTO VALOR|15876|\n",
      "|         VALOR MEDIO|15245|\n",
      "|          BAJO VALOR|14925|\n",
      "+--------------------+-----+\n",
      "\n",
      "\n",
      "✔ Ejemplo de clientes por cada categoría:\n",
      "+------------+-----------------+--------------------+\n",
      "|CLIENT_ID   |TOTAL_SPEND      |ECONOMIC_VALUE_CLASS|\n",
      "+------------+-----------------+--------------------+\n",
      "|ES182348429U|192187.19        |ALTO VALOR          |\n",
      "|ES182227107Z|144442.25        |ALTO VALOR          |\n",
      "|ES182129030R|120642.64        |ALTO VALOR          |\n",
      "|ES182117163D|100618.2         |ALTO VALOR          |\n",
      "|ES182403153O|99603.0          |ALTO VALOR          |\n",
      "|ES182262674U|91530.0          |ALTO VALOR          |\n",
      "|ES182446227G|87831.59000000003|ALTO VALOR          |\n",
      "|ES182116963C|84592.95         |ALTO VALOR          |\n",
      "|ES182132454N|84581.42000000001|ALTO VALOR          |\n",
      "|ES182423117N|84021.48999999999|ALTO VALOR          |\n",
      "|ES182215757Z|83904.73         |ALTO VALOR          |\n",
      "|ES182366160A|82594.29000000001|ALTO VALOR          |\n",
      "|ES182358570I|79456.63         |ALTO VALOR          |\n",
      "|ES182177455I|77063.4          |ALTO VALOR          |\n",
      "|ES182156500S|76723.06         |ALTO VALOR          |\n",
      "|ES182173073E|75471.23000000001|ALTO VALOR          |\n",
      "|ES182166373H|75064.03000000001|ALTO VALOR          |\n",
      "|ES182363662D|73749.7          |ALTO VALOR          |\n",
      "|ES182412497V|73249.06         |ALTO VALOR          |\n",
      "|ES182254874N|73201.85         |ALTO VALOR          |\n",
      "+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. CLASIFICACIÓN DE VALOR ECONÓMICO (E-SCORE: Alto, Medio, Bajo)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Índices de \"customer value\" (cuartiles o percentiles)\n",
    "# Usaremos los tertiles (33% y 66%) de TOTAL_SPEND para la segmentación.\n",
    "quantiles = econ.approxQuantile(\"TOTAL_SPEND\", [0.33, 0.66], 0.01)\n",
    "p33, p66 = quantiles\n",
    "\n",
    "econ = econ.withColumn(\n",
    "    \"ECONOMIC_VALUE_CLASS\",\n",
    "    F.when(F.col(\"TOTAL_SPEND\") >= p66, \"ALTO VALOR\")\n",
    "     .when(F.col(\"TOTAL_SPEND\") >= p33, \"VALOR MEDIO\")\n",
    "     .otherwise(\"BAJO VALOR\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ 3. SEGMENTACIÓN DE VALOR ECONÓMICO (E-SCORE) ================\\n\")\n",
    "print(\"Segmentación (Alto, Medio, Bajo) basada en los tertiles de Gasto Total (TOTAL_SPEND).\")\n",
    "\n",
    "print(\"\\n✔ Segmentación económica completada:\")\n",
    "econ.groupBy(\"ECONOMIC_VALUE_CLASS\").count().show()\n",
    "\n",
    "print(\"\\n✔ Ejemplo de clientes por cada categoría:\")\n",
    "econ.select(\"CLIENT_ID\", \"TOTAL_SPEND\", \"ECONOMIC_VALUE_CLASS\") \\\n",
    "    .orderBy(F.col(\"TOTAL_SPEND\").desc()).show(20, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "914afc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 4. UNIÓN AL MASTER ================\n",
      "\n",
      "Métricas económicas añadidas al master correctamente.\n",
      "Nuevo total de columnas: 58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. UNIÓN AL MASTER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_master = df_master.join(econ, on=\"CLIENT_ID\", how=\"left\")\n",
    "\n",
    "print(\"\\n================ 4. UNIÓN AL MASTER ================\\n\")\n",
    "print(\"Métricas económicas añadidas al master correctamente.\")\n",
    "print(f\"Nuevo total de columnas: {len(df_master.columns)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bfa53",
   "metadata": {},
   "source": [
    "### INTERACCIÓN Y FIDELIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d9aac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. PREPARACIÓN DE DATOS BASE\n",
    "# ASUMIMOS:\n",
    "# - 'beh' (DataFrame BEHAVIOURAL) está cargado y limpio.\n",
    "# - 'df_master' (DataFrame CLIENTS/Master) está cargado.\n",
    "# ==========================================\n",
    "\n",
    "# Rellenar Nulos en las columnas de gasto y pago.\n",
    "# Es fundamental tratar los valores nulos como 0 antes de realizar sumas o calcular ratios para evitar errores (NaN) e inconsistencias.\n",
    "beh = beh.fillna(0, subset=[\"CREDIT_CARD_DRAWINGS_ATM\", \"CREDIT_CARD_DRAWINGS_POS\", \"CREDIT_CARD_DRAWINGS_OTHER\",\"CREDIT_CARD_PAYMENT\", \"CREDIT_CARD_DRAWINGS\", \"NUMBER_DRAWINGS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e82cf8",
   "metadata": {},
   "source": [
    "1. Métricas de Uso de Canales y Fidelidad Transaccional\n",
    "\n",
    "1.1. Cálculo de Gasto Total y Agregación\n",
    "\n",
    "Se calcula el gasto total por transacción (KPI_TOTAL_SPEND) y luego se agregan todas las transacciones por cliente para obtener los totales (sumas) de gasto por canal y de pagos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "54b6d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.1. KPI de Gasto Total (Total Drawings) por Transacción ---\n",
    "# Se crea la columna de Gasto Total sumando los gastos por cada canal.\n",
    "if \"KPI_TOTAL_SPEND\" not in beh.columns:\n",
    "    beh = beh.withColumn(\"KPI_TOTAL_SPEND\", F.col(\"CREDIT_CARD_DRAWINGS_ATM\") + F.col(\"CREDIT_CARD_DRAWINGS_POS\") + F.col(\"CREDIT_CARD_DRAWINGS_OTHER\"))\n",
    "\n",
    "\n",
    "# --- 1.2. Métrica de Repuesta/Fidelidad por CLIENTE (Agregación) ---\n",
    "\n",
    "interaccion_df = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    # Suma de Gasto por Canales\n",
    "    F.sum(\"CREDIT_CARD_DRAWINGS_ATM\").alias(\"SPEND_ATM_SUM\"),\n",
    "    F.sum(\"CREDIT_CARD_DRAWINGS_POS\").alias(\"SPEND_POS_SUM\"),\n",
    "    F.sum(\"CREDIT_CARD_DRAWINGS_OTHER\").alias(\"SPEND_OTHER_SUM\"),\n",
    "\n",
    "    # Suma de Gasto Total y Pago Total\n",
    "    F.sum(\"KPI_TOTAL_SPEND\").alias(\"TOTAL_DRAWINGS_SPEND\"),\n",
    "    F.sum(\"CREDIT_CARD_PAYMENT\").alias(\"TOTAL_PAYMENTS\") # Dinero que paga (amortiza)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcd31a",
   "metadata": {},
   "source": [
    "1.2. Cálculo de Ratios (Manejo de División por Cero)\n",
    "Para evitar errores de división por cero (que ocurrían si TOTAL_DRAWINGS_SPEND era 0), se utiliza la técnica de sumar + 1 al denominador en todos los cálculos de ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "568d17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de Ratios\n",
    "# NOTA: Se mantiene la técnica de sumar +1 al denominador para evitar división por cero (y NaN/Inf)\n",
    "interaccion_df = interaccion_df.withColumn(\n",
    "    # Ratio ATM: Proporción del gasto en ATM sobre el Gasto Total.\n",
    "    \"CHANNEL_ATM_RATIO\", F.round(F.col(\"SPEND_ATM_SUM\") / (F.col(\"TOTAL_DRAWINGS_SPEND\") + 1), 4)\n",
    ").withColumn(\n",
    "    # Ratio POS: Proporción del gasto en POS sobre el Gasto Total.\n",
    "    \"CHANNEL_POS_RATIO\", F.round(F.col(\"SPEND_POS_SUM\") / (F.col(\"TOTAL_DRAWINGS_SPEND\") + 1), 4)\n",
    ").withColumn(\n",
    "    # Ratio OTHER: Proporción del gasto en Otros sobre el Gasto Total.\n",
    "    \"CHANNEL_OTHER_RATIO\", F.round(F.col(\"SPEND_OTHER_SUM\") / (F.col(\"TOTAL_DRAWINGS_SPEND\") + 1), 4)\n",
    ").withColumn(\n",
    "    # Métrica de Fidelidad (Gasto vs. Pago): Mide el uso responsable. Valores altos implican que el cliente paga mucho en relación a su gasto.\n",
    "    \"PAYMENT_FIDELITY_RATIO\", F.round(F.col(\"TOTAL_PAYMENTS\") / (F.col(\"TOTAL_DRAWINGS_SPEND\") + 1), 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0d3a6",
   "metadata": {},
   "source": [
    "1.3. Top 10 Clientes Fieles\n",
    "\n",
    "Este paso implementa la corrección para obtener un Top 10 de clientes fieles que realmente están activos y gastando, lo que garantiza que los ratios de canal sean informativos y no 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a9ced38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Top 10 Clientes con Mayor Fidelidad de Pago (FILTRANDO los clientes con GASTO CERO):\n",
      "+------------+-------------+-------------+---------------+--------------------+------------------+-----------------+-----------------+-------------------+----------------------+\n",
      "|CLIENT_ID   |SPEND_ATM_SUM|SPEND_POS_SUM|SPEND_OTHER_SUM|TOTAL_DRAWINGS_SPEND|TOTAL_PAYMENTS    |CHANNEL_ATM_RATIO|CHANNEL_POS_RATIO|CHANNEL_OTHER_RATIO|PAYMENT_FIDELITY_RATIO|\n",
      "+------------+-------------+-------------+---------------+--------------------+------------------+-----------------+-----------------+-------------------+----------------------+\n",
      "|ES182216044T|0.0          |2.05         |0.0            |2.05                |1241.95           |0.0              |0.6721           |0.0                |407.1967              |\n",
      "|ES182423065Q|10.8         |0.0          |0.0            |10.8                |3135.15           |0.9153           |0.0              |0.0                |265.6907              |\n",
      "|ES182215735T|27.0         |0.0          |0.0            |27.0                |6848.869999999993 |0.9643           |0.0              |0.0                |244.6025              |\n",
      "|ES182243846V|5.4          |0.0          |0.0            |5.4                 |1381.699999999998 |0.8438           |0.0              |0.0                |215.8906              |\n",
      "|ES182221875P|0.0          |9.13         |0.0            |9.13                |2047.0500000000018|0.0              |0.9013           |0.0                |202.078               |\n",
      "|ES182232609P|0.0          |15.05        |0.0            |15.05               |2100.2299999999977|0.0              |0.9377           |0.0                |130.8555              |\n",
      "|ES182164679N|0.0          |0.0          |160.38         |160.38              |18396.249999999996|0.0              |0.0              |0.9938             |113.9934              |\n",
      "|ES182393047B|54.0         |0.0          |0.0            |54.0                |6131.7000000000035|0.9818           |0.0              |0.0                |111.4855              |\n",
      "|ES182183423O|27.0         |0.0          |0.0            |27.0                |2448.519999999997 |0.9643           |0.0              |0.0                |87.4471               |\n",
      "|ES182193556E|27.0         |0.0          |0.0            |27.0                |2342.109999999999 |0.9643           |0.0              |0.0                |83.6468               |\n",
      "+------------+-------------+-------------+---------------+--------------------+------------------+-----------------+-----------------+-------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1.3. Top 10 Clientes Fieles (y Activos) ---\n",
    "print(\"\\n✔ Top 10 Clientes con Mayor Fidelidad de Pago (FILTRANDO los clientes con GASTO CERO):\")\n",
    "# FILTRO CRÍTICO: Excluimos a los clientes cuyo gasto total es 0. \n",
    "# Esto elimina los 'outliers' de fidelidad artificial y asegura que los ratios de canal (ATM, POS, Other) sean significativos (> 0).\n",
    "interaccion_df.filter(F.col(\"TOTAL_DRAWINGS_SPEND\") > 0)\\\n",
    "    .orderBy(F.col(\"PAYMENT_FIDELITY_RATIO\").desc())\\\n",
    "    .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba7d20",
   "metadata": {},
   "source": [
    "2. Diversidad de Interacción\n",
    "\n",
    "Se calcula un Score de Diversidad contando cuántos tipos de canales de gasto (ATM, POS, OTHER) el cliente ha utilizado al menos una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fb9596f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 2. DIVERSIDAD DE CANALES (REPETICIÓN) ================\n",
      "\n",
      "Score: 1 punto por cada tipo de gasto de tarjeta utilizado (ATM, POS, Other).\n",
      "+-----------------------+-----+\n",
      "|CHANNEL_DIVERSITY_SCORE|count|\n",
      "+-----------------------+-----+\n",
      "|                      0|14652|\n",
      "|                      1|15582|\n",
      "|                      2|14730|\n",
      "|                      3| 1082|\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1. Diversidad de Canales (Repetición/Uso) ---\n",
    "# Contar los tipos de transacciones con gasto en la ventana\n",
    "beh_diversity = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    # Contamos el número de transacciones con gasto en cada canal\n",
    "    F.sum(F.when(F.col(\"CREDIT_CARD_DRAWINGS_ATM\") > 0, 1).otherwise(0)).alias(\"NUM_ATM_TXN\"),\n",
    "    F.sum(F.when(F.col(\"CREDIT_CARD_DRAWINGS_POS\") > 0, 1).otherwise(0)).alias(\"NUM_POS_TXN\"),\n",
    "    F.sum(F.when(F.col(\"CREDIT_CARD_DRAWINGS_OTHER\") > 0, 1).otherwise(0)).alias(\"NUM_OTHER_TXN\")\n",
    ").withColumn(\n",
    "    \"CHANNEL_DIVERSITY_SCORE\",\n",
    "    # CHANNEL_DIVERSITY_SCORE: Suma 1 punto por cada tipo de canal que tenga al menos una transacción (TXN > 0).\n",
    "    F.when(F.col(\"NUM_ATM_TXN\") > 0, 1).otherwise(0) +\n",
    "    F.when(F.col(\"NUM_POS_TXN\") > 0, 1).otherwise(0) +\n",
    "    F.when(F.col(\"NUM_OTHER_TXN\") > 0, 1).otherwise(0)\n",
    ").drop(\"NUM_ATM_TXN\", \"NUM_POS_TXN\", \"NUM_OTHER_TXN\") # Mantenemos el score final\n",
    "\n",
    "\n",
    "# Unimos el score al DataFrame de métricas de interacción\n",
    "interaccion_df = interaccion_df.join(beh_diversity, \"CLIENT_ID\", \"left\")\n",
    "\n",
    "print(\"\\n================ 2. DIVERSIDAD DE CANALES (REPETICIÓN) ================\\n\")\n",
    "print(\"Score: 1 punto por cada tipo de gasto de tarjeta utilizado (ATM, POS, Other).\")\n",
    "interaccion_df.groupBy(\"CHANNEL_DIVERSITY_SCORE\").count().orderBy(\"CHANNEL_DIVERSITY_SCORE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730703cd",
   "metadata": {},
   "source": [
    "3. Segmentación de Interacción (I-Score)\n",
    "\n",
    "Se combinan el Ratio de Fidelidad y el Score de Diversidad para crear una segmentación de tres niveles (Alta, Media, Baja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3594bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 3. SEGMENTACIÓN FINAL DE INTERACCIÓN (I-SCORE) ================\n",
      "\n",
      "+-------------------+-----+\n",
      "|INTERACTION_SEGMENT|count|\n",
      "+-------------------+-----+\n",
      "|               Alta| 5620|\n",
      "|              Media|26900|\n",
      "|               Baja|13526|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. SEGMENTACIÓN DE INTERACCIÓN (I-SCORE)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# I-Score: Mide la calidad (Fidelidad) y la amplitud (Diversidad) de la interacción.\n",
    "\n",
    "# Definir umbrales de Fidelidad usando percentiles (33% y 66%).\n",
    "fidelidad_quantiles = interaccion_df.approxQuantile(\"PAYMENT_FIDELITY_RATIO\", [0.33, 0.66], 0.01)\n",
    "f_p33, f_p66 = fidelidad_quantiles\n",
    "\n",
    "# Umbral de Diversidad: Usar al menos 2 canales (score >= 2) se considera Alta Diversidad.\n",
    "diversidad_umbral = 2\n",
    "\n",
    "interaccion_df = interaccion_df.withColumn(\n",
    "    \"INTERACTION_SEGMENT\",\n",
    "    # Segmento ALTA: Cumple con alta fidelidad (percentil 66) Y alta diversidad (2 o 3 canales).\n",
    "    F.when((F.col(\"PAYMENT_FIDELITY_RATIO\") >= f_p66) & (F.col(\"CHANNEL_DIVERSITY_SCORE\") >= diversidad_umbral), \"Alta\")\n",
    "     # Segmento MEDIA: Cumple con media fidelidad (percentil 33) O ha usado al menos 1 canal.\n",
    "     .when((F.col(\"PAYMENT_FIDELITY_RATIO\") >= f_p33) | (F.col(\"CHANNEL_DIVERSITY_SCORE\") >= 1), \"Media\")\n",
    "     # Segmento BAJA: Resto de clientes que tienen baja fidelidad y/o nula diversidad de canales.\n",
    "     .otherwise(\"Baja\") \n",
    ")\n",
    "\n",
    "print(\"\\n================ 3. SEGMENTACIÓN FINAL DE INTERACCIÓN (I-SCORE) ================\\n\")\n",
    "interaccion_df.groupBy(\"INTERACTION_SEGMENT\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996427b",
   "metadata": {},
   "source": [
    "4. Unión al DataFrame Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7f47927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 4. UNIÓN AL MASTER ================\n",
      "\n",
      "Métricas de interacción y fidelidad añadidas al master correctamente.\n",
      "Nuevo total de columnas: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. UNIÓN AL MASTER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Seleccionamos solo las columnas de métricas finales para la unión, descartando auxiliares.\n",
    "cols_to_keep = [\"CLIENT_ID\", \"CHANNEL_ATM_RATIO\", \"CHANNEL_POS_RATIO\", \"CHANNEL_OTHER_RATIO\",\n",
    "                \"PAYMENT_FIDELITY_RATIO\", \"CHANNEL_DIVERSITY_SCORE\", \"INTERACTION_SEGMENT\"]\n",
    "\n",
    "interaccion_df_final = interaccion_df.select(*cols_to_keep)\n",
    "\n",
    "# Unimos las métricas al DataFrame maestro de clientes (`df_master`) mediante el CLIENT_ID.\n",
    "df_master = df_master.join(interaccion_df_final, on=\"CLIENT_ID\", how=\"left\")\n",
    "\n",
    "print(\"\\n================ 4. UNIÓN AL MASTER ================\\n\")\n",
    "print(\"Métricas de interacción y fidelidad añadidas al master correctamente.\")\n",
    "print(f\"Nuevo total de columnas: {len(df_master.columns)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347eb1fe",
   "metadata": {},
   "source": [
    "### RIESGO POTENCIAL (abandono e inactividad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278ebbb",
   "metadata": {},
   "source": [
    "Esta sección se centra en evaluar la **salud y el compromiso** del cliente mediante métricas temporales que identifican a aquellos con mayor riesgo de abandono (*Churn*). Utilizamos los pilares del modelo RFM (Recencia, Frecuencia).\n",
    "\n",
    "### 1. Métricas Clave\n",
    "\n",
    "Las métricas se calculan utilizando la columna `DATE` del historial transaccional (`beh`) como referencia.\n",
    "\n",
    "| Métrica | Definición | Interpretación en Riesgo |\n",
    "| :--- | :--- | :--- |\n",
    "| **`LAST_TXN_DATE`** | Fecha de la última interacción o transacción registrada. | Punto de partida para el cálculo de la recencia. |\n",
    "| **`DAYS_SINCE_LAST_TXN`** | **Recencia**. Número de días transcurridos entre la última actividad y la fecha de análisis (`max_date`). | **Riesgo:** Valor alto $\\rightarrow$ Mayor riesgo de abandono. |\n",
    "| **`TOTAL_TXN_COUNT`** | **Frecuencia Bruta**. Número total de transacciones (gastos y pagos) realizadas por el cliente. | **Compromiso:** Valor alto $\\rightarrow$ Mayor compromiso. |\n",
    "| **`SPEND_FREQUENCY_RATIO`** | **Propensión al Gasto**. Proporción de transacciones que fueron gasto sobre el total de transacciones. | **Salud:** Sugiere si el cliente usa el producto activamente para gastar o solo para amortizar deuda. |\n",
    "\n",
    "### 2. Segmentación de Riesgo\n",
    "\n",
    "Clasificamos a los clientes en segmentos de riesgo basados en los días de inactividad, que es el indicador más directo de abandono (*Churn*).\n",
    "\n",
    "| Segmento | Criterio (`INACTIVITY_DAYS`) | Implicación de Negocio |\n",
    "| :--- | :--- | :--- |\n",
    "| **Bajo** | $< 30$ días | **Cliente activo.** No requiere intervención de retención. |\n",
    "| **Medio** | $30-89$ días | **Inactivo Reciente**. El cliente ha reducido su actividad y es candidato a campañas de reactivación. |\n",
    "| **Alto** | $\\ge 90$ días | **Abandono Potencial**. El cliente ha dejado de usar el producto. Requiere una estrategia de recuperación urgente. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "780b80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la fecha de análisis (Max Date)\n",
    "max_date = beh.agg(F.max(\"DATE\")).first()[0]\n",
    "MAX_DATE_LIT = F.lit(max_date)\n",
    "\n",
    "# Calcular KPI_TOTAL_SPEND en el nivel transaccional (beh)\n",
    "# Es necesario para la métrica de SPEND_FREQUENCY_RATIO y coherente con el módulo económico.\n",
    "beh = beh.withColumn(\n",
    "    \"KPI_TOTAL_SPEND\",\n",
    "    F.col(\"CREDIT_CARD_DRAWINGS_ATM\") + F.col(\"CREDIT_CARD_DRAWINGS_POS\") + F.col(\"CREDIT_CARD_DRAWINGS_OTHER\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82e20321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 1. INDICADORES DE RECENCIA Y ACTIVIDAD ================\n",
      "\n",
      "+-------+-------------------+---------------------+\n",
      "|summary|DAYS_SINCE_LAST_TXN|SPEND_FREQUENCY_RATIO|\n",
      "+-------+-------------------+---------------------+\n",
      "|  count|              46046|                46046|\n",
      "|   mean| 15.444794336098685|  0.24331314989358707|\n",
      "| stddev|  21.54619528266416|   0.2986459125125185|\n",
      "|    min|                  0|                  0.0|\n",
      "|    max|                 93|                  1.0|\n",
      "+-------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. MÉTRICAS DE RIESGO POTENCIAL (RECENCIA Y FRECUENCIA)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Agregamos los datos a nivel de CLIENT_ID.\n",
    "riesgo_df = beh.groupBy(\"CLIENT_ID\").agg(\n",
    "    # Recencia: Fecha de la última transacción.\n",
    "    F.max(\"DATE\").alias(\"LAST_TXN_DATE\"), \n",
    "\n",
    "    # Frecuencia Bruta: Número total de movimientos (gastos + pagos).\n",
    "    F.count(\"*\").alias(\"TOTAL_TXN_COUNT\"),\n",
    "\n",
    "    # Frecuencia de Gasto: Número de veces que el cliente tuvo un gasto > 0.\n",
    "    F.sum(F.when(F.col(\"KPI_TOTAL_SPEND\") > 0, 1).otherwise(0)).alias(\"SPEND_TXN_COUNT\")\n",
    ")\n",
    "\n",
    "\n",
    "# --- 1.1. Cálculo de Recencia, Inactividad y Ratio de Gasto ---\n",
    "\n",
    "riesgo_df = riesgo_df.withColumn(\n",
    "    # 1. DAYS_SINCE_LAST_TXN (Recencia): Días transcurridos desde la última actividad.\n",
    "    \"DAYS_SINCE_LAST_TXN\",\n",
    "    F.datediff(MAX_DATE_LIT, F.col(\"LAST_TXN_DATE\"))\n",
    ").withColumn(\n",
    "    # 2. INACTIVITY_DAYS: Usamos la recencia como indicador de inactividad.\n",
    "    \"INACTIVITY_DAYS\",\n",
    "    F.col(\"DAYS_SINCE_LAST_TXN\")\n",
    ").withColumn(\n",
    "    # 3. SPEND_FREQUENCY_RATIO: Proporción de las transacciones que fueron de gasto.\n",
    "    \"SPEND_FREQUENCY_RATIO\",\n",
    "    F.round(F.col(\"SPEND_TXN_COUNT\") / F.col(\"TOTAL_TXN_COUNT\"), 4)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n================ 1. INDICADORES DE RECENCIA Y ACTIVIDAD ================\\n\")\n",
    "riesgo_df.select(\"DAYS_SINCE_LAST_TXN\", \"SPEND_FREQUENCY_RATIO\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8f629369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 2. SEGMENTACIÓN DE RIESGO DE ABANDONO ================\n",
      "\n",
      "+------------+-----+\n",
      "|RISK_SEGMENT|count|\n",
      "+------------+-----+\n",
      "|       Medio|17575|\n",
      "|        Alto|  864|\n",
      "|        Bajo|27607|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "--- Muestra de clientes segmentados por Riesgo ---\n",
      "+------------+-------------+-------------------+------------+\n",
      "|   CLIENT_ID|LAST_TXN_DATE|DAYS_SINCE_LAST_TXN|RISK_SEGMENT|\n",
      "+------------+-------------+-------------------+------------+\n",
      "|ES182406226A|   2021-09-29|                 93|        Alto|\n",
      "|ES182205817D|   2021-09-29|                 93|        Alto|\n",
      "|ES182211003Y|   2021-09-29|                 93|        Alto|\n",
      "|ES182451631D|   2021-09-29|                 93|        Alto|\n",
      "|ES182406265H|   2021-09-29|                 93|        Alto|\n",
      "|ES182405527S|   2021-09-29|                 93|        Alto|\n",
      "|ES182211280N|   2021-09-29|                 93|        Alto|\n",
      "|ES182179190D|   2021-09-29|                 93|        Alto|\n",
      "|ES182306422P|   2021-09-29|                 93|        Alto|\n",
      "|ES182109223R|   2021-09-29|                 93|        Alto|\n",
      "+------------+-------------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. SEGMENTACIÓN DE RIESGO (BASADO EN INACTIVIDAD)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Definición de umbrales: 30 días para Inactivo, 90 días para Abandono (Alto Riesgo).\n",
    "INACTIVE_THRESHOLD = 30 \n",
    "CHURN_THRESHOLD = 90    \n",
    "\n",
    "riesgo_df = riesgo_df.withColumn(\n",
    "    \"RISK_SEGMENT\",\n",
    "    F.when(F.col(\"INACTIVITY_DAYS\") >= CHURN_THRESHOLD, \"Alto\")\n",
    "     .when(F.col(\"INACTIVITY_DAYS\") >= INACTIVE_THRESHOLD, \"Medio\")\n",
    "     .otherwise(\"Bajo\")\n",
    ")\n",
    "\n",
    "print(\"\\n================ 2. SEGMENTACIÓN DE RIESGO DE ABANDONO ================\\n\")\n",
    "riesgo_df.groupBy(\"RISK_SEGMENT\").count().show()\n",
    "\n",
    "print(\"\\n--- Muestra de clientes segmentados por Riesgo ---\")\n",
    "riesgo_df.select(\"CLIENT_ID\", \"LAST_TXN_DATE\", \"DAYS_SINCE_LAST_TXN\", \"RISK_SEGMENT\")\\\n",
    "         .orderBy(F.col(\"DAYS_SINCE_LAST_TXN\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3ca78255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 3. UNIÓN AL MASTER ================\n",
      "\n",
      "Métricas de riesgo potencial añadidas al master correctamente.\n",
      "Nuevo total de columnas: 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. UNIÓN AL MASTER\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Cargar el master (curated) para asegurar la continuidad.\n",
    "df_master = spark.read.parquet(DATA_PATH + \"curated/Master_FinPlus.parquet\")\n",
    "\n",
    "cols_to_keep_risk = [\n",
    "    \"CLIENT_ID\",\n",
    "    \"LAST_TXN_DATE\",\n",
    "    \"DAYS_SINCE_LAST_TXN\",\n",
    "    \"TOTAL_TXN_COUNT\",\n",
    "    \"SPEND_FREQUENCY_RATIO\",\n",
    "    \"RISK_SEGMENT\"\n",
    "]\n",
    "\n",
    "riesgo_df_final = riesgo_df.select(*cols_to_keep_risk)\n",
    "\n",
    "df_master = df_master.join(riesgo_df_final, on=\"CLIENT_ID\", how=\"left\")\n",
    "\n",
    "print(\"\\n================ 3. UNIÓN AL MASTER ================\\n\")\n",
    "print(\"Métricas de riesgo potencial añadidas al master correctamente.\")\n",
    "print(f\"Nuevo total de columnas: {len(df_master.columns)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dd554",
   "metadata": {},
   "source": [
    "### OPORTUNIDADES COMERCIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc1785",
   "metadata": {},
   "source": [
    "El objetivo de este módulo de Oportunidades Comerciales es identificar a los clientes que tienen un alto potencial económico (alto valor), pero que actualmente están infrautilizando nuestros productos o canales (baja interacción). Estos clientes representan la mayor oportunidad de crecimiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aad576",
   "metadata": {},
   "source": [
    "\n",
    "Este módulo combina el **Valor Económico (E-Score)** con la **Interacción (I-Score)** para identificar a los clientes con mayor potencial de crecimiento. El enfoque se centra en la matriz **\"Potencial vs. Penetración\"**.\n",
    "\n",
    "El segmento de mayor interés es el de **Oportunidad Comercial Alta**: Clientes con la Capacidad de Gasto para ser rentables, pero que aún no han sido activados o están usando pocos canales.\n",
    "\n",
    "### 1. Segmentos Clave\n",
    "\n",
    "Para este análisis, utilizamos los segmentos ya calculados:\n",
    "\n",
    "| Segmento Clave | Descripción |\n",
    "| :--- | :--- |\n",
    "| **`ECONOMIC_VALUE_CLASS`** | Clasificación del cliente basada en el **Gasto Total** (Alto, Medio, Bajo). |\n",
    "| **`INTERACTION_SEGMENT`** | Clasificación del cliente basada en la **Fidelidad y Diversidad de Canales** (Alta, Media, Baja). |\n",
    "\n",
    "### 2. Matriz de Oportunidad\n",
    "\n",
    "Definimos la oportunidad comercial (potencial de crecimiento) mediante las siguientes reglas:\n",
    "\n",
    "| Oportunidad | Criterio | Acción Comercial |\n",
    "| :--- | :--- | :--- |\n",
    "| **Alta Oportunidad** | **VALOR ALTO** y **INTERACCIÓN BAJA/MEDIA** | **Activación/Cross-selling.** Incentivar el uso de más canales o productos.  |\n",
    "| **Retención/Fidelidad** | **VALOR ALTO** e **INTERACCIÓN ALTA** | **Recompensa/Fidelización.** Clientes que deben ser mantenidos y premiados. |\n",
    "| **Inversión Limitada** | **VALOR BAJO** o **VALOR MEDIO** | **Monitoreo/Eficiencia.** Baja prioridad para campañas costosas. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "936438d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 0. PREPARACIÓN DE DATOS Y GARANTÍA DE COLUMNAS\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Usamos la opción mergeSchema=true si la ruta es correcta\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_master \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMaster_FinPlus.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Definir columnas de segmentación que podrían estar duplicadas en el master.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m COLS_SEGMENTATION \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECONOMIC_VALUE_CLASS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTERACTION_SEGMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMMERCIAL_OPPORTUNITY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0. PREPARACIÓN DE DATOS Y GARANTÍA DE COLUMNAS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Usamos la opción mergeSchema=true si la ruta es correcta\n",
    "df_master = spark.read.option(\"mergeSchema\", \"true\").parquet(OUTPUT_PATH + \"Master_FinPlus.parquet\")\n",
    "\n",
    "# 1. Definir columnas de segmentación que podrían estar duplicadas en el master.\n",
    "COLS_SEGMENTATION = [\"ECONOMIC_VALUE_CLASS\", \"INTERACTION_SEGMENT\", \"COMMERCIAL_OPPORTUNITY\"]\n",
    "COLS_METRICS = [\"TOTAL_SPEND\"] # Métrica económica necesaria para el análisis Top 5\n",
    "\n",
    "# 2. PASO CRÍTICO: ELIMINAR COLUMNAS DUPLICADAS Y DE SEGMENTACIÓN EXISTENTES DEL MASTER\n",
    "COLS_TO_DROP = COLS_SEGMENTATION + COLS_METRICS\n",
    "df_master = df_master.drop(*COLS_TO_DROP)\n",
    "\n",
    "\n",
    "# --- UNIÓN TEMPORAL DE SEGMENTOS CLAVE ---\n",
    "# 3. Unir el segmento de VALOR al master limpio (INCLUIMOS TOTAL_SPEND)\n",
    "df_temp = df_master.join(\n",
    "    econ.select(\"CLIENT_ID\", \"ECONOMIC_VALUE_CLASS\", \"TOTAL_SPEND\"), # <--- CORRECCIÓN CLAVE\n",
    "    on=\"CLIENT_ID\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Unir el segmento de INTERACCIÓN al master temporal\n",
    "df_temp = df_temp.join(\n",
    "    interaccion_df.select(\"CLIENT_ID\", \"INTERACTION_SEGMENT\"), \n",
    "    on=\"CLIENT_ID\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5. Rellenar nulos de clientes sin actividad transaccional\n",
    "df_temp = df_temp.fillna(\"BAJO VALOR\", subset=[\"ECONOMIC_VALUE_CLASS\"])\n",
    "df_temp = df_temp.fillna(\"Baja\", subset=[\"INTERACTION_SEGMENT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "40bd46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 1. CREACIÓN DE MATRIZ DE OPORTUNIDAD ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. SEGMENTACIÓN DE OPORTUNIDAD COMERCIAL (MATRIZ)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n================ 1. CREACIÓN DE MATRIZ DE OPORTUNIDAD ================\\n\")\n",
    "\n",
    "# Combinamos los segmentos de Valor Económico y Fidelidad de Interacción.\n",
    "oportunidad_df = df_temp.withColumn(\n",
    "    \"COMMERCIAL_OPPORTUNITY\",\n",
    "    # OPORTUNIDAD ALTA: Clientes con ALTO VALOR, pero BAJA o MEDIA INTERACCIÓN/DIVERSIDAD.\n",
    "    F.when((F.col(\"ECONOMIC_VALUE_CLASS\") == \"ALTO VALOR\") & (F.col(\"INTERACTION_SEGMENT\").isin(\"Baja\", \"Media\")), \"Oportunidad Alta\")\n",
    "     \n",
    "    # FIDELIDAD/RETENCIÓN: Clientes con ALTO VALOR y ALTA INTERACCIÓN.\n",
    "    .when((F.col(\"ECONOMIC_VALUE_CLASS\") == \"ALTO VALOR\") & (F.col(\"INTERACTION_SEGMENT\") == \"Alta\"), \"Retención/Fidelidad\")\n",
    "    \n",
    "    # RIESGO DE MIGRACIÓN: Clientes de VALOR MEDIO/BAJO con ALTA INTERACCIÓN (podrían estar migrando).\n",
    "    .when((F.col(\"ECONOMIC_VALUE_CLASS\").isin(\"VALOR MEDIO\", \"BAJO VALOR\")) & (F.col(\"INTERACTION_SEGMENT\") == \"Alta\"), \"Riesgo de Migración\")\n",
    "    \n",
    "    # BAJA INVERSIÓN: Resto de combinaciones.\n",
    "    .otherwise(\"Baja Inversión\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e117d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Distribución de Clientes por Segmento de Oportunidad:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8256.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1246.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1246.0 (TID 3516) (368bd66ad6da executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/data/curated/Master_FinPlus.parquet/part-00006-c02b006b-6d1b-4323-9f86-e0c7b6b24ead-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/data/curated/Master_FinPlus.parquet/part-00006-c02b006b-6d1b-4323-9f86-e0c7b6b24ead-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 2. ANÁLISIS Y RESULTADOS\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔ Distribución de Clientes por Segmento de Oportunidad:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43moportunidad_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCOMMERCIAL_OPPORTUNITY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✔ Top 5 Clientes de \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOportunidad Alta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (Clientes Valiosos Infrautilizados):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m oportunidad_df\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMMERCIAL_OPPORTUNITY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOportunidad Alta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m               \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIENT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOTAL_INCOME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOTAL_SPEND\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECONOMIC_VALUE_CLASS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTERACTION_SEGMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     11\u001b[0m               \u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOTAL_INCOME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8256.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1246.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1246.0 (TID 3516) (368bd66ad6da executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/data/curated/Master_FinPlus.parquet/part-00006-c02b006b-6d1b-4323-9f86-e0c7b6b24ead-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/data/curated/Master_FinPlus.parquet/part-00006-c02b006b-6d1b-4323-9f86-e0c7b6b24ead-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. ANÁLISIS Y RESULTADOS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"✔ Distribución de Clientes por Segmento de Oportunidad:\")\n",
    "oportunidad_df.groupBy(\"COMMERCIAL_OPPORTUNITY\").count().orderBy(F.col(\"count\").desc()).show(truncate=False)\n",
    "\n",
    "print(\"\\n✔ Top 5 Clientes de 'Oportunidad Alta' (Clientes Valiosos Infrautilizados):\")\n",
    "oportunidad_df.filter(F.col(\"COMMERCIAL_OPPORTUNITY\") == \"Oportunidad Alta\")\\\n",
    "              .select(\"CLIENT_ID\", \"TOTAL_INCOME\", \"TOTAL_SPEND\", \"ECONOMIC_VALUE_CLASS\", \"INTERACTION_SEGMENT\")\\\n",
    "              .orderBy(F.col(\"TOTAL_INCOME\").desc()).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "246262ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `ECONOMIC_VALUE_CLASS` is ambiguous, could be: [`ECONOMIC_VALUE_CLASS`, `ECONOMIC_VALUE_CLASS`, `ECONOMIC_VALUE_CLASS`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m df_master_final \u001b[38;5;241m=\u001b[39m df_master\u001b[38;5;241m.\u001b[39mjoin(oportunidad_df_final, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIENT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 3. PASO CRÍTICO DE PERSISTENCIA: SELECCIÓN EXPLÍCITA DE COLUMNAS ANTES DE ESCRIBIR\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Este paso es la solución definitiva a los Py4JJavaErrors durante la escritura.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forzamos a Spark a construir un esquema limpio.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df_master_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf_master_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_master_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 4. Guardado final (Sobreescribir el Master)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df_master_clean\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(OUTPUT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaster_FinPlus.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `ECONOMIC_VALUE_CLASS` is ambiguous, could be: [`ECONOMIC_VALUE_CLASS`, `ECONOMIC_VALUE_CLASS`, `ECONOMIC_VALUE_CLASS`]."
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. UNIÓN AL MASTER (PERSISTENCIA FINAL)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1. Seleccionamos las columnas finales y las segmentaciones para persistir\n",
    "cols_to_persist = [\"CLIENT_ID\"] + COLS_SEGMENTATION + COLS_METRICS \n",
    "oportunidad_df_final = oportunidad_df.select(*cols_to_persist)\n",
    "\n",
    "# 2. Unimos la segmentación al df_master (que no tiene las columnas duplicadas)\n",
    "# df_master fue cargado limpio en el paso 0\n",
    "df_master_final = df_master.join(oportunidad_df_final, on=\"CLIENT_ID\", how=\"left\")\n",
    "\n",
    "\n",
    "# 3. PASO CRÍTICO DE PERSISTENCIA: SELECCIÓN EXPLÍCITA DE COLUMNAS ANTES DE ESCRIBIR\n",
    "# Este paso es la solución definitiva a los Py4JJavaErrors durante la escritura.\n",
    "# Forzamos a Spark a construir un esquema limpio.\n",
    "df_master_clean = df_master_final.select(df_master_final.columns)\n",
    "\n",
    "# 4. Guardado final (Sobreescribir el Master)\n",
    "df_master_clean.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"Master_FinPlus.parquet\")\n",
    "\n",
    "\n",
    "print(\"\\n================ 3. UNIÓN AL MASTER ================\\n\")\n",
    "print(\"Segmentación de Oportunidad Comercial y métricas clave añadidas al master y persistidas correctamente.\")\n",
    "print(f\"Nuevo total de columnas: {len(df_master_clean.columns)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bbeb0",
   "metadata": {},
   "source": [
    "### ANÁLISIS DE CAUSALIDAD / UPLIFT (para identificar qué ofertas realmente causan más retención)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273ecd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97971f30",
   "metadata": {},
   "source": [
    "### EMBEDDING DE COMPORTAMIENTO - SEQUENCE MODELING - (para recomendar productos RNNs / transformers si hay consecuencias largas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ef984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bc7331a",
   "metadata": {},
   "source": [
    "### ANOMALÍA TRANSACCIONAL (para detectar fraudes o glitches del sistema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a64d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
